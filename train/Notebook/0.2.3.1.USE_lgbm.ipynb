{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# manipulation des données\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# matplotlib et seaborn pour les représentations graphiques\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../Data/2.train_df.csv\")\n",
    "df = df.dropna(subset=['words'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-15 12:58:26.325580: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "# import tensorflow_hub as hub\n",
    "import tensorflow.keras\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras import metrics as kmetrics\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "os.environ[\"TF_KERAS\"]='1'\n",
    "os.environ[\"TFHUB_MODEL_LOAD_FORMAT\"] = \"UNCOMPRESSED\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-15 12:58:31.159670: W external/local_tsl/tsl/platform/cloud/google_auth_provider.cc:184] All attempts to get a Google authentication bearer token failed, returning an empty token. Retrieving token from files failed with \"NOT_FOUND: Could not locate the credentials file.\". Retrieving token from GCE failed with \"FAILED_PRECONDITION: Error executing an HTTP request: libcurl code 6 meaning 'Couldn't resolve host name', error details: Could not resolve host: metadata.google.internal\".\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import shutil\n",
    "\n",
    "embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_USE_fct(sentences, b_size) :\n",
    "    batch_size = b_size\n",
    "\n",
    "    features = None\n",
    "\n",
    "    for step in range(len(sentences)//batch_size) :\n",
    "        idx = step*batch_size\n",
    "        feat = embed(sentences[idx:idx+batch_size])\n",
    "\n",
    "        if step ==0 :\n",
    "            features = feat\n",
    "        else :\n",
    "            features = np.concatenate((features,feat))\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "sentences = df['words'].sample(100000, random_state=42)\n",
    "sentences = sentences.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_sentences = feature_USE_fct(sentences, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "labels = df['target'].sample(100000, random_state=42)\n",
    "labels = labels.values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(x_sentences, labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_sets = [\n",
    "    {'max_depth': 10},\n",
    "    {'max_depth': 15},\n",
    "    {'num_leaves': 12},\n",
    "    {'num_leaves': 32},\n",
    "    {'feature_fraction': 0.6},\n",
    "    {'feature_fraction': 0.8},\n",
    "    {'boosting': 'gbdt'},\n",
    "    {'boosting': 'dart'}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 39985, number of negative: 40015\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.191810 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 130560\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 512\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499812 -> initscore=-0.000750\n",
      "[LightGBM] [Info] Start training from score -0.000750\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/envs/Projet7bisEnv/lib/python3.10/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.\n",
      "  warnings.warn(\"Setuptools is replacing distutils.\")\n",
      "Successfully registered model 'LightGBM_USE'.\n",
      "Created version '1' of model 'LightGBM_USE'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 39985, number of negative: 40015\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.186439 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 130560\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 512\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499812 -> initscore=-0.000750\n",
      "[LightGBM] [Info] Start training from score -0.000750\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 39985, number of negative: 40015\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.236010 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 130560\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 512\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499812 -> initscore=-0.000750\n",
      "[LightGBM] [Info] Start training from score -0.000750\n",
      "[LightGBM] [Info] Number of positive: 39985, number of negative: 40015\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.201302 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 130560\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 512\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499812 -> initscore=-0.000750\n",
      "[LightGBM] [Info] Start training from score -0.000750\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Info] Number of positive: 39985, number of negative: 40015\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.169028 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 130560\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 512\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499812 -> initscore=-0.000750\n",
      "[LightGBM] [Info] Start training from score -0.000750\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Info] Number of positive: 39985, number of negative: 40015\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.197764 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 130560\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 512\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499812 -> initscore=-0.000750\n",
      "[LightGBM] [Info] Start training from score -0.000750\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Registered model 'LightGBM_USE' already exists. Creating a new version of this model...\n",
      "Created version '2' of model 'LightGBM_USE'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "[LightGBM] [Info] Number of positive: 39985, number of negative: 40015\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.204746 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 130560\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 512\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499812 -> initscore=-0.000750\n",
      "[LightGBM] [Info] Start training from score -0.000750\n",
      "[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "[LightGBM] [Warning] boosting is set=dart, boosting_type=gbdt will be ignored. Current value: boosting=dart\n",
      "[LightGBM] [Warning] boosting is set=dart, boosting_type=gbdt will be ignored. Current value: boosting=dart\n",
      "[LightGBM] [Info] Number of positive: 39985, number of negative: 40015\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.198434 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 130560\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 512\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499812 -> initscore=-0.000750\n",
      "[LightGBM] [Info] Start training from score -0.000750\n",
      "[LightGBM] [Warning] boosting is set=dart, boosting_type=gbdt will be ignored. Current value: boosting=dart\n",
      "[LightGBM] [Warning] boosting is set=dart, boosting_type=gbdt will be ignored. Current value: boosting=dart\n",
      "[LightGBM] [Warning] boosting is set=dart, boosting_type=gbdt will be ignored. Current value: boosting=dart\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import adjusted_rand_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_curve, auc, confusion_matrix\n",
    "\n",
    "artifact_path = './artifacts/'\n",
    "\n",
    "best_metric_value = -1\n",
    "best_model_path = \"\"\n",
    "\n",
    "mlflow.set_experiment(\"DeepLearningTweet\")\n",
    "\n",
    "for i, params in enumerate(param_sets):\n",
    "    name_experience = f'{list(params.keys())[0]}_{list(params.values())[0]}' # héhéhéhé ça marche\n",
    "    with mlflow.start_run(run_name=f\"USE_LightGBM_{name_experience}\"):\n",
    "        clf = lgb.LGBMClassifier(**params)\n",
    "        clf.fit(X_train, y_train)\n",
    "\n",
    "        y_pred = clf.predict(X_test)\n",
    "        mlflow.log_param(\"params\", params)\n",
    "        mlflow.log_metric(\"accuracy\", clf.score(X_test, y_test))\n",
    "        mlflow.log_metric(\"Precision\", precision_score(y_test, y_pred))\n",
    "        mlflow.log_metric(\"Recall\", recall_score(y_test, y_pred))\n",
    "        mlflow.log_metric(\"F1_Score\", f1_score(y_test, y_pred))\n",
    "\n",
    "        f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "        fpr, tpr, thresholds = roc_curve(y_test, clf.predict_proba(X_test)[:, 1])\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        mlflow.log_metric(\"AUC\", roc_auc)\n",
    "\n",
    "        if f1 > best_metric_value:\n",
    "            best_metric_value = f1\n",
    "            mlflow.set_tag(\"tag1\", \"LightGBM with USE\")\n",
    "            mlflow.set_tags({\"tag2\":f'{name_experience}'})\n",
    "            mlflow.sklearn.log_model(clf, \"model\", registered_model_name=\"LightGBM_USE\")\n",
    "\n",
    "            conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "            conf_matrix_path = f\"{artifact_path}confMat_USE_LightGBM_{name_experience}.csv\"\n",
    "            pd.DataFrame(conf_matrix).to_csv(conf_matrix_path, index=False, header=False)\n",
    "            mlflow.log_artifact(conf_matrix_path, \"metrics\")\n",
    "\n",
    "            plt.figure(figsize=(8, 8))\n",
    "            plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = {:.2f})'.format(roc_auc))\n",
    "            plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "            plt.xlabel('False Positive Rate')\n",
    "            plt.ylabel('True Positive Rate')\n",
    "            plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "            plt.legend(loc=\"lower right\")\n",
    "            plt.grid(True)\n",
    "            roc_curve_path = f\"{artifact_path}roc_USE_LightGBM_{name_experience}.png\"\n",
    "            plt.savefig(roc_curve_path)\n",
    "            plt.close()\n",
    "            mlflow.log_artifact(roc_curve_path, \"plots\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Projet7Env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
