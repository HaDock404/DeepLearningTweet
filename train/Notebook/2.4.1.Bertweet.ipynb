{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# manipulation des données\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# matplotlib et seaborn pour les représentations graphiques\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../Data/2.sample_dataset.csv\")\n",
    "df = df.dropna(subset=['words'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_1 = df.sample(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target\n",
       "0    515\n",
       "1    485\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_1['target'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "emoji is not installed, thus not converting emoticons or emojis into text. Install emoji: pip3 install emoji==0.6.0\n"
     ]
    }
   ],
   "source": [
    "bertweet = AutoModel.from_pretrained(\"vinai/bertweet-base\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"vinai/bertweet-base\", use_fast=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'I am so happy to see you at this evening!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'I am so happy to see you at this evening!'\n",
    "tokens = tokenizer.encode(text, add_special_tokens=True, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = bertweet(tokens)\n",
    "logits = outputs.last_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative [7.72545682e-05 7.71932100e-05 8.43600355e-05 5.98084735e-05\n",
      " 7.24764541e-05 6.10682691e-05 7.04932536e-05 6.75996853e-05\n",
      " 7.32230546e-05 6.11402284e-05 7.56897571e-05 8.89040966e-05\n",
      " 7.68745231e-05 6.78662473e-05 6.50031725e-05 5.97713261e-05\n",
      " 7.65511286e-05 7.94790249e-05 1.00997102e-04 1.00720026e-04\n",
      " 7.99291520e-05 3.35034274e-05 6.60709120e-05 6.14701203e-05\n",
      " 7.96413879e-05 6.13674056e-05 7.05234415e-05 6.00313979e-05\n",
      " 5.74419864e-05 7.58035821e-05 7.25310456e-05 9.17117941e-05\n",
      " 4.47180755e-05 5.44028408e-05 1.04835388e-04 6.46141343e-05\n",
      " 7.19554519e-05 7.41391996e-05 9.45953871e-05 7.51319021e-05\n",
      " 8.08186742e-05 5.32518352e-05 8.89372677e-05 5.25089563e-05\n",
      " 8.36387044e-05 6.98679360e-05 8.32034639e-05 6.27435074e-05\n",
      " 9.52660994e-05 8.27984986e-05 7.59521427e-05 9.40113969e-05\n",
      " 7.45744546e-05 7.48314342e-05 5.70206576e-05 3.54170043e-05\n",
      " 6.18549748e-05 1.07577718e-04 1.00742953e-04 9.95799855e-05\n",
      " 6.20244828e-05 6.73120448e-05 9.44128260e-05 9.40033206e-05\n",
      " 1.18210017e-04 7.22723926e-05 8.01214701e-05 5.72165663e-05\n",
      " 7.03467158e-05 9.09902228e-05 5.37709857e-05 4.58465183e-05\n",
      " 8.01152855e-05 8.43373564e-05 4.17349329e-05 6.65245898e-05\n",
      " 7.93677755e-05 7.82162097e-05 6.05721398e-05 6.30336217e-05\n",
      " 6.50846487e-05 6.72800161e-05 6.48317437e-05 8.41458314e-05\n",
      " 7.01928802e-05 5.79198459e-05 7.85454904e-05 8.31076613e-05\n",
      " 8.68597999e-05 6.86491767e-05 8.94472032e-05 7.45713260e-05\n",
      " 6.48610221e-05 5.76091988e-05 7.07307263e-05 7.70437327e-05\n",
      " 5.76753337e-05 6.47230772e-05 5.61618253e-05 9.73950519e-05\n",
      " 5.30459984e-05 6.34151002e-05 7.57107409e-05 7.86957025e-05\n",
      " 1.16553660e-04 9.26600842e-05 8.28588891e-05 4.16451330e-05\n",
      " 6.34464668e-05 9.68528257e-05 8.86782727e-05 6.22708321e-05\n",
      " 6.00046624e-05 6.64472027e-05 4.30356449e-05 6.22899825e-05\n",
      " 6.85655614e-05 7.92793871e-05 6.26111723e-05 5.21562579e-05\n",
      " 8.21747308e-05 9.75525181e-05 3.37584534e-05 6.72120732e-05\n",
      " 6.73117829e-05 1.10482666e-04 7.86419550e-05 5.77088867e-05\n",
      " 1.30856526e-04 7.24106867e-05 7.61599440e-05 6.91282039e-05\n",
      " 6.87320062e-05 7.92832434e-05 9.83436476e-05 9.56737495e-05\n",
      " 7.34947535e-05 6.86462590e-05 8.68091665e-05 7.79594047e-05\n",
      " 7.92014835e-05 9.00027517e-05 8.38701453e-05 7.71571576e-05\n",
      " 7.52799024e-05 7.69885446e-05 4.88956939e-05 9.22440522e-05\n",
      " 9.12007308e-05 9.05801498e-05 7.68974642e-05 9.42652841e-05\n",
      " 6.08431183e-05 6.03487970e-05 7.32464905e-05 8.02096620e-05\n",
      " 7.04975901e-05 7.86921082e-05 6.87758802e-05 4.84485427e-05\n",
      " 5.22298978e-05 7.37829905e-05 8.03831790e-05 9.66578518e-05\n",
      " 6.76668715e-05 5.81451459e-05 7.04055055e-05 8.59999782e-05\n",
      " 8.51441291e-05 5.07842633e-05 7.83098803e-05 1.04337880e-04\n",
      " 6.01019638e-05 6.47480192e-05 5.14378771e-05 1.02757789e-04\n",
      " 7.71522900e-05 6.40267972e-05 9.01422536e-05 9.44363346e-05\n",
      " 7.88497127e-05 6.86563071e-05 8.50613942e-05 4.45184050e-05\n",
      " 7.71267296e-05 1.23727761e-04 3.85255407e-05 8.01339338e-05\n",
      " 9.07097274e-05 5.50886616e-05 4.89380764e-05 5.97047219e-05\n",
      " 9.51022084e-05 8.09850972e-05 1.13630129e-04 8.79564177e-05\n",
      " 5.88546754e-05 7.89049300e-05 6.67686982e-05 7.03255500e-05\n",
      " 7.99115151e-05 7.50290783e-05 5.79942352e-05 7.35953290e-05\n",
      " 6.14687742e-05 8.50626020e-05 8.57977211e-05 8.12534199e-05\n",
      " 7.22606055e-05 5.96549544e-05 8.02839713e-05 6.73920658e-05\n",
      " 6.56685006e-05 6.80177400e-05 6.55622498e-05 3.58753641e-06\n",
      " 5.22907540e-05 6.77405042e-05 7.66111843e-05 7.12504407e-05\n",
      " 6.01490683e-05 8.73145545e-05 7.24929414e-05 5.84155714e-05\n",
      " 7.70835613e-05 5.37030028e-05 7.18922383e-05 8.19341367e-05\n",
      " 4.79277696e-05 8.34082311e-05 9.56858494e-05 1.06931380e-04\n",
      " 6.62053208e-05 5.67717325e-05 7.77790992e-05 8.38388733e-05\n",
      " 5.33356215e-05 7.99820700e-05 5.99922423e-05 4.85546225e-05\n",
      " 7.73936044e-05 8.34935126e-05 1.09491186e-04 8.24607559e-05\n",
      " 7.42420380e-05 7.53314307e-05 9.26168868e-05 1.40618315e-04\n",
      " 8.38391570e-05 8.87176429e-05 9.99956610e-05 6.54280157e-05\n",
      " 7.33903944e-05 7.89410187e-05 1.15583942e-04 8.71426455e-05\n",
      " 8.42622321e-05 7.35883441e-05 8.52969242e-05 6.97369906e-05\n",
      " 8.22403090e-05 1.10056761e-04 6.85230043e-05 7.77315217e-05\n",
      " 8.01697024e-05 7.15920178e-05 8.55042381e-05 7.84155200e-05\n",
      " 8.07662364e-05 6.55624390e-05 8.54719110e-05 5.23066374e-05\n",
      " 9.33774791e-05 6.21527943e-05 8.62437955e-05 1.00431425e-04\n",
      " 5.49609867e-05 1.02850841e-04 6.34981334e-05 5.32440681e-05\n",
      " 1.22873782e-04 4.55350710e-05 7.72932472e-05 7.65377263e-05\n",
      " 8.73236349e-05 5.93327168e-05 5.98709667e-05 1.14211587e-04\n",
      " 6.19059647e-05 1.30299522e-04 9.15427008e-05 6.67900895e-05\n",
      " 4.55499685e-05 9.34244672e-05 6.60796723e-05 1.01855112e-04\n",
      " 6.78659926e-05 6.66528067e-05 7.54945140e-05 5.41676600e-05\n",
      " 1.06244173e-04 8.79541549e-05 5.11949656e-05 8.58051644e-05\n",
      " 5.54633662e-05 7.50156978e-05 7.44486169e-05 7.67025340e-05\n",
      " 8.04887095e-05 5.16313667e-05 4.70260529e-05 6.00827952e-05\n",
      " 1.46474471e-04 5.63213944e-05 6.42660234e-05 9.99753029e-05\n",
      " 6.51727169e-05 8.11246937e-05 6.99535085e-05 8.44855240e-05\n",
      " 8.56721381e-05 7.30153624e-05 7.69437611e-05 5.55968400e-05\n",
      " 7.31868276e-05 6.36488257e-05 7.80980336e-05 1.15531824e-04\n",
      " 6.71917151e-05 7.89146798e-05 6.89097069e-05 6.45677719e-05\n",
      " 7.79368420e-05 7.09474552e-05 5.39863031e-05 7.73010688e-05\n",
      " 8.09205958e-05 7.59458781e-05 5.08207959e-05 6.52480958e-05\n",
      " 7.70958504e-05 8.80301814e-05 8.48410637e-05 2.49610275e-05\n",
      " 1.17264644e-04 7.78630783e-05 5.18151137e-05 1.27804218e-04\n",
      " 1.04145467e-04 5.86232927e-05 9.69057219e-05 4.49432155e-05\n",
      " 1.38979318e-04 7.55345536e-05 7.28463856e-05 1.17821714e-04\n",
      " 6.20506835e-05 7.66871817e-05 5.71848177e-05 1.01596088e-04\n",
      " 3.33816715e-05 7.99323534e-05 6.74498078e-05 9.93853464e-05\n",
      " 6.43974054e-05 5.67436109e-05 7.03989936e-05 7.05058919e-05\n",
      " 7.27891966e-05 5.53738100e-05 6.22586813e-05 6.06931062e-05\n",
      " 8.47724732e-05 7.91033381e-05 8.40874272e-05 6.32080264e-05\n",
      " 3.19494538e-05 1.48638001e-05 9.18021833e-05 9.95595110e-05\n",
      " 1.03192404e-04 7.52344276e-05 8.89322546e-05 6.61963277e-05\n",
      " 7.59702089e-05 7.69648177e-05 7.40897885e-05 7.56165682e-05\n",
      " 6.76763520e-05 1.08655440e-04 5.94140220e-05 7.26275684e-05\n",
      " 6.22353255e-05 6.65823827e-05 9.75750299e-05 5.46019182e-05\n",
      " 8.13217557e-05 8.06540193e-05 9.20901948e-05 7.86598393e-05\n",
      " 6.52229064e-05 7.61662595e-05 8.79045110e-05 5.37317137e-05\n",
      " 1.26941712e-04 8.06045064e-05 7.86036762e-05 9.69690373e-05\n",
      " 9.36225988e-05 6.55120239e-05 7.50012987e-05 6.44291067e-05\n",
      " 4.69387051e-05 7.00026576e-05 6.83459075e-05 8.42868976e-05\n",
      " 8.04111332e-05 8.64586909e-05 8.16696338e-05 8.17400432e-05\n",
      " 8.73367826e-05 6.99211232e-05 7.03207916e-05 7.71523264e-05\n",
      " 5.73977668e-05 6.39557620e-05 1.05234059e-04 6.70737281e-05\n",
      " 9.66298758e-05 5.82858920e-05 5.77768151e-05 6.75896372e-05\n",
      " 7.54995199e-05 8.24716481e-05 1.24193772e-04 6.73358954e-05\n",
      " 8.20728310e-05 9.85133738e-05 9.40449318e-05 9.69229513e-05\n",
      " 8.57187915e-05 7.02323232e-05 7.65436052e-05 7.20852986e-05\n",
      " 6.78866636e-05 7.71257374e-03 6.31093135e-05 1.04831386e-04\n",
      " 7.88096731e-05 1.11227666e-04 5.77097708e-05 9.36740835e-05\n",
      " 8.40477878e-05 7.48403472e-05 1.03669809e-04 5.25750474e-05\n",
      " 8.34422826e-05 5.29649406e-05 5.78480758e-05 5.14527383e-05\n",
      " 9.79725373e-05 8.58579297e-05 7.41719050e-05 1.00896876e-04\n",
      " 7.45815341e-05 5.94266567e-05 9.48331653e-05 5.66189665e-05\n",
      " 5.08277481e-05 6.13994634e-05 6.35244214e-05 6.06796202e-05\n",
      " 6.46327171e-05 4.86824356e-05 7.21755860e-05 8.39170316e-05\n",
      " 7.56597074e-05 7.39751340e-05 5.13799278e-05 9.91412526e-05\n",
      " 6.70474110e-05 6.60901933e-05 7.60702242e-05 1.20532641e-04\n",
      " 7.54151770e-05 7.39170646e-05 1.13714901e-04 8.90657175e-05\n",
      " 7.78036920e-05 6.04491361e-05 9.51854963e-05 1.11293040e-04\n",
      " 1.05581465e-04 3.93494302e-05 7.02768302e-05 6.32109877e-05\n",
      " 6.43425883e-05 5.38466484e-05 7.16436189e-05 8.25323077e-05\n",
      " 6.56419797e-05 6.14217788e-05 7.34974819e-05 8.36577383e-05\n",
      " 7.85384473e-05 7.89534388e-05 6.56881675e-05 8.68590578e-05\n",
      " 5.17037042e-05 4.50790758e-05 5.72912868e-05 5.75652630e-05\n",
      " 6.80012963e-05 1.20091245e-04 6.23369560e-05 6.84419574e-05\n",
      " 7.00286982e-05 7.12525070e-05 1.34666960e-04 7.01318932e-05\n",
      " 5.62473761e-05 6.34473763e-05 4.74386579e-05 6.50513248e-05\n",
      " 3.72788199e-05 9.97868847e-05 5.59021755e-05 7.81504932e-05\n",
      " 7.04878767e-05 7.31910477e-05 8.57818450e-05 6.01616302e-05\n",
      " 5.57732565e-05 2.47417775e-04 5.58291067e-05 6.74039620e-05\n",
      " 6.85542473e-05 1.12985836e-04 7.45352081e-05 5.33994207e-05\n",
      " 1.27914464e-04 7.13017289e-05 8.95154735e-05 1.11673064e-04\n",
      " 6.59680300e-05 7.19782693e-05 7.53754139e-05 7.39117750e-05\n",
      " 7.33970446e-05 8.74034886e-05 6.21362269e-05 6.91035821e-05\n",
      " 5.30597354e-05 7.17225194e-05 8.11569698e-05 4.10720641e-05\n",
      " 8.59853826e-05 1.00285950e-04 4.70133455e-05 6.63821265e-05\n",
      " 6.17187761e-05 6.77539501e-05 1.00851183e-04 6.13152006e-05\n",
      " 4.68119470e-05 7.08231673e-05 8.24306844e-05 9.66774896e-05\n",
      " 6.80624071e-05 9.85345541e-05 7.06772262e-05 7.70799597e-05\n",
      " 4.67567552e-05 6.47600318e-05 1.25922248e-04 8.16357206e-05\n",
      " 5.29672398e-05 9.45746797e-05 7.47217127e-05 4.78156908e-05\n",
      " 9.03220571e-05 5.07280638e-05 7.36324000e-05 8.82284439e-05\n",
      " 6.15885656e-05 5.61243214e-05 8.02297509e-05 5.98804654e-05\n",
      " 7.84288422e-05 5.32769045e-05 1.06891770e-04 1.27120031e-04\n",
      " 1.23997917e-04 9.88495813e-05 1.07175532e-04 7.51034604e-05\n",
      " 6.43589665e-05 7.90197300e-05 1.06028128e-04 5.81331660e-05\n",
      " 4.90087696e-05 6.87237844e-05 6.91023015e-05 1.06715452e-04\n",
      " 7.66902158e-05 7.50989129e-05 6.02691325e-05 6.20071514e-05\n",
      " 6.25328830e-05 8.45880277e-05 5.88411785e-05 6.10167990e-05\n",
      " 5.98054794e-05 9.61799742e-05 7.67174643e-05 7.94325606e-05\n",
      " 8.53088422e-05 5.82057291e-05 6.46968474e-05 6.25781395e-05\n",
      " 8.20068672e-05 5.08675112e-05 6.19678103e-05 6.97431096e-05\n",
      " 5.57012572e-05 8.00710914e-05 9.44005005e-05 6.20833598e-05\n",
      " 6.38346755e-05 6.80850353e-05 9.34478012e-05 9.90213448e-05\n",
      " 5.08247213e-05 8.65161783e-05 1.01217229e-04 4.94255182e-05\n",
      " 7.45244397e-05 6.33352320e-05 7.29258318e-05 7.69624312e-05\n",
      " 8.02917057e-05 8.15635067e-05 8.29198761e-05 8.20481873e-05\n",
      " 6.70570298e-05 4.29881838e-05 5.77679748e-05 8.42739610e-05\n",
      " 9.75374060e-05 6.27787667e-05 1.08292086e-04 1.03406426e-04\n",
      " 5.52569036e-05 6.18096237e-05 5.18342968e-05 9.02906540e-05\n",
      " 8.00996277e-05 7.00450619e-05 6.39777209e-05 5.99169725e-05\n",
      " 1.40863529e-04 7.96309760e-05 7.51325133e-05 7.52065316e-05\n",
      " 7.38333983e-05 1.10712441e-04 7.56590598e-05 5.61651141e-05\n",
      " 1.01175974e-04 4.53230205e-05 4.51344895e-05 8.30618228e-05\n",
      " 9.10681410e-05 6.47139386e-05 6.01326101e-05 6.25073662e-05\n",
      " 7.11409230e-05 9.60494508e-05 1.00541285e-04 9.53813142e-05\n",
      " 6.18734121e-05 6.68438079e-05 7.10196473e-05 4.35940201e-05\n",
      " 9.62470949e-05 1.09132845e-04 9.34196869e-05 6.69754445e-05\n",
      " 6.57890414e-05 6.46147746e-05 7.10089153e-05 8.55969993e-05\n",
      " 6.79847290e-05 7.10194799e-05 7.93724394e-05 7.72107669e-05\n",
      " 6.22850202e-05 6.77399585e-05 7.33342094e-05 4.61655764e-05\n",
      " 7.16916038e-05 7.88146353e-05 6.10510251e-05 1.24465558e-04\n",
      " 7.15803835e-05 8.55596663e-05 6.14850360e-05 9.70005058e-05\n",
      " 5.97691032e-05 4.76371351e-05 6.14670716e-05 1.39081676e-04\n",
      " 7.58618626e-05 9.32498660e-05 9.19291269e-05 7.39282405e-05\n",
      " 6.55541517e-05 8.66370028e-05 9.28148438e-05 6.57723504e-05\n",
      " 1.13811191e-04 5.31869227e-05 5.73715770e-05 5.15237225e-05\n",
      " 7.22570185e-05 1.10099398e-04 8.21512222e-05 8.68484203e-05\n",
      " 6.43511739e-05 6.21705403e-05 6.39645077e-05 7.46007718e-05\n",
      " 5.51867561e-05 5.94873927e-05 5.61955203e-05 5.73413308e-05\n",
      " 7.97059329e-05 7.70228580e-05 7.10716486e-05 5.59662185e-05\n",
      " 7.82174102e-05 4.88489968e-05 6.31653384e-05 7.79060429e-05\n",
      " 7.07804647e-05 1.43104524e-04 8.49233911e-05 5.72938297e-05\n",
      " 5.77231185e-05 7.34365021e-05 9.63660350e-05 7.11550601e-05\n",
      " 8.39262430e-05 6.25123721e-05 6.35956749e-05 9.39515739e-05\n",
      " 8.18758199e-05 9.97659517e-05 7.54812936e-05 7.06846768e-05\n",
      " 7.89928672e-05 7.29183594e-05 7.26516082e-05 7.14487251e-05\n",
      " 8.12357175e-05 6.60672304e-05 7.10323147e-05 6.50275106e-05]\n",
      "Neutral [1.24857033e-04 7.05327620e-05 1.07826025e-04 8.28412667e-05\n",
      " 5.97358558e-05 6.66555134e-05 5.14375315e-05 8.91907621e-05\n",
      " 1.28150481e-04 6.54803298e-05 9.15636119e-05 9.02415995e-05\n",
      " 8.85453410e-05 6.50333022e-05 4.08945452e-05 6.50147922e-05\n",
      " 5.44657632e-05 8.16388856e-05 8.72249293e-05 7.45559009e-05\n",
      " 6.45426626e-05 7.12326000e-05 8.50699071e-05 6.40903600e-05\n",
      " 8.45546383e-05 6.31306175e-05 6.98213771e-05 6.95298659e-05\n",
      " 6.96446441e-05 7.19544914e-05 4.89821796e-05 4.97207839e-05\n",
      " 6.28290800e-05 7.82303905e-05 8.92565367e-05 7.30763859e-05\n",
      " 7.27319202e-05 4.25621365e-05 4.61236777e-05 8.81190499e-05\n",
      " 5.92757606e-05 8.29680066e-05 8.92144599e-05 7.20407625e-05\n",
      " 1.23900812e-04 1.00072553e-04 6.00772109e-05 6.43706371e-05\n",
      " 6.15777317e-05 9.10822564e-05 6.22492735e-05 1.44153761e-04\n",
      " 2.86251219e-04 8.68095667e-05 8.28937045e-05 7.48434904e-05\n",
      " 1.14147449e-04 7.58118913e-05 9.77785821e-05 7.18300143e-05\n",
      " 6.87913198e-05 6.05940950e-05 1.01466379e-04 7.83505238e-05\n",
      " 7.32954359e-05 6.76674535e-05 9.33297997e-05 5.61740162e-05\n",
      " 4.34834474e-05 6.23202213e-05 6.15311728e-05 7.34668938e-05\n",
      " 6.74203256e-05 7.87330137e-05 8.73357494e-05 6.92021567e-05\n",
      " 6.90094748e-05 3.44348337e-05 6.04576999e-05 8.10213678e-05\n",
      " 6.93065958e-05 9.88180982e-05 5.50825425e-05 5.80915221e-05\n",
      " 5.87695031e-05 8.55559556e-05 7.19302698e-05 1.23961159e-04\n",
      " 1.25208491e-04 5.49260112e-05 6.81294623e-05 1.01026664e-04\n",
      " 4.92307408e-05 4.35810289e-05 4.43425997e-05 7.81909403e-05\n",
      " 5.53498066e-05 4.03972554e-05 5.78233121e-05 9.20895254e-05\n",
      " 5.49423094e-05 7.56435547e-05 7.06963328e-05 7.59338509e-05\n",
      " 8.65672191e-05 5.80922424e-05 8.59990832e-05 5.01689137e-05\n",
      " 5.65427690e-05 7.49267201e-05 5.76467319e-05 4.92669715e-05\n",
      " 6.10238712e-05 5.78164763e-05 5.45974399e-05 7.81320487e-05\n",
      " 7.46385267e-05 8.27119293e-05 1.13719834e-04 6.84596380e-05\n",
      " 8.30654972e-05 6.88900254e-05 1.13716036e-04 9.30226088e-05\n",
      " 6.20193605e-05 1.47959945e-04 1.03437342e-04 8.03244111e-05\n",
      " 4.35541224e-05 6.61369486e-05 5.72016470e-05 7.45616853e-05\n",
      " 5.67029601e-05 7.09032174e-05 9.08540533e-05 6.26710025e-05\n",
      " 7.78010581e-05 8.30512145e-05 4.15717222e-05 6.82915997e-05\n",
      " 8.57068299e-05 6.70398585e-05 8.49239150e-05 8.40936482e-05\n",
      " 6.18091290e-05 7.04406557e-05 6.94347764e-05 6.79654040e-05\n",
      " 7.30699758e-05 8.87715942e-05 7.99705522e-05 8.44443130e-05\n",
      " 6.65108309e-05 8.20969872e-05 9.67590749e-05 5.78017789e-05\n",
      " 9.18554360e-05 5.56809719e-05 9.14524862e-05 6.94939663e-05\n",
      " 1.01918711e-04 6.51620285e-05 6.80943485e-05 6.98894873e-05\n",
      " 1.01367776e-04 7.98008114e-05 5.73979305e-05 8.56781771e-05\n",
      " 9.08194124e-05 3.45711196e-05 1.47094310e-04 6.81037709e-05\n",
      " 8.26852993e-05 7.32500848e-05 7.42620396e-05 9.54443676e-05\n",
      " 6.75253395e-05 8.39195927e-05 6.61460654e-05 7.50116960e-05\n",
      " 9.28316294e-05 7.29441308e-05 7.49004685e-05 4.88766054e-05\n",
      " 6.43722888e-05 8.67344788e-05 5.82765497e-05 6.91815949e-05\n",
      " 6.79417135e-05 1.24670652e-04 5.17030603e-05 8.02391587e-05\n",
      " 7.89594560e-05 8.07446049e-05 9.50180256e-05 9.18858277e-05\n",
      " 7.72567291e-05 8.66325499e-05 7.23272387e-05 6.69950896e-05\n",
      " 1.06166590e-04 7.26778744e-05 6.34021926e-05 4.94257074e-05\n",
      " 6.93709808e-05 9.87809326e-05 5.33497150e-05 9.51092225e-05\n",
      " 5.28934106e-05 7.24881029e-05 5.73750513e-05 8.18734770e-05\n",
      " 5.53146383e-05 1.38598727e-04 8.06619137e-05 8.37856904e-03\n",
      " 8.56029583e-05 6.65496918e-05 6.80221492e-05 1.02034050e-04\n",
      " 6.92524918e-05 8.28985649e-05 1.11149056e-04 5.61808447e-05\n",
      " 6.88579457e-05 6.65632397e-05 5.63020840e-05 5.82947796e-05\n",
      " 4.11725196e-05 7.50538456e-05 8.25821480e-05 5.07128243e-05\n",
      " 9.55254727e-05 6.42195155e-05 9.38147641e-05 6.44094398e-05\n",
      " 8.42478039e-05 7.36638976e-05 1.05556399e-04 9.36470678e-05\n",
      " 6.84215629e-05 7.18069350e-05 8.31556172e-05 6.35669639e-05\n",
      " 9.27027722e-05 8.39574568e-05 8.84326728e-05 9.61615879e-05\n",
      " 9.37065706e-05 1.59378265e-04 1.07211818e-04 5.89278534e-05\n",
      " 7.14799680e-05 5.44815812e-05 7.81270937e-05 9.26690918e-05\n",
      " 6.86738931e-05 8.03544826e-05 7.08163789e-05 6.87021893e-05\n",
      " 6.41963416e-05 8.09888370e-05 7.56708541e-05 7.64193537e-05\n",
      " 8.89720031e-05 7.25598293e-05 8.57502382e-05 6.00750936e-05\n",
      " 6.40964427e-05 1.24142272e-04 6.41942970e-05 6.76830095e-05\n",
      " 8.03760922e-05 7.13823465e-05 6.65820335e-05 7.98609253e-05\n",
      " 2.17899636e-04 7.88773978e-05 9.98519099e-05 8.65912079e-05\n",
      " 1.34429763e-04 1.04857230e-04 7.16442373e-05 6.59414291e-05\n",
      " 6.12954973e-05 7.06169958e-05 7.30824104e-05 6.11819924e-05\n",
      " 7.71349296e-05 5.59485488e-05 8.16878601e-05 9.19543745e-05\n",
      " 8.33070299e-05 9.15839628e-05 6.06883041e-05 7.91466955e-05\n",
      " 7.08698135e-05 4.64803998e-05 6.69520086e-05 8.82630338e-05\n",
      " 1.05261119e-04 4.83311560e-05 5.58948232e-05 1.11951864e-04\n",
      " 8.86164707e-05 9.87793319e-05 7.63803328e-05 8.51213554e-05\n",
      " 1.12981907e-04 7.88510952e-05 7.20936878e-05 4.69624138e-05\n",
      " 6.84209372e-05 5.92054785e-05 8.13840961e-05 7.75457956e-05\n",
      " 6.58370918e-05 8.21095964e-05 5.96066457e-05 9.93641152e-05\n",
      " 6.94979462e-05 4.67637365e-05 7.72412604e-05 1.08294931e-04\n",
      " 6.22849620e-05 7.37838491e-05 6.74998082e-05 7.45328289e-05\n",
      " 7.57098023e-05 7.76377710e-05 5.08834237e-05 1.08769440e-04\n",
      " 8.29146520e-05 1.67309568e-04 6.77513890e-05 6.91008245e-05\n",
      " 1.01101170e-04 9.53531635e-05 7.63289645e-05 6.82516547e-05\n",
      " 9.25261265e-05 5.30268044e-05 8.29458513e-05 1.04094524e-05\n",
      " 1.09377484e-04 7.73120410e-05 7.07461077e-05 9.38712328e-05\n",
      " 1.28770727e-04 6.50342990e-05 7.17318107e-05 5.54207181e-05\n",
      " 6.83660182e-05 6.99533412e-05 8.49555508e-05 8.86525595e-05\n",
      " 9.66951557e-05 7.59526738e-05 5.88076000e-05 5.72788849e-05\n",
      " 6.26720212e-05 5.98545994e-05 7.32357730e-05 7.12018737e-05\n",
      " 1.00246303e-04 8.77063358e-05 9.22007821e-05 8.15586463e-05\n",
      " 7.55040091e-05 6.19807179e-05 4.64484037e-05 4.30954387e-05\n",
      " 1.41542900e-04 6.98987933e-05 9.72044581e-05 4.00730823e-05\n",
      " 4.79421979e-05 7.20493204e-04 7.02838734e-05 7.00064920e-05\n",
      " 5.69224976e-05 7.92793508e-05 7.89393234e-05 8.92513053e-05\n",
      " 8.64133181e-05 8.62979359e-05 5.83278415e-05 8.52330122e-05\n",
      " 5.94420198e-05 5.36482585e-05 7.05934217e-05 6.72370006e-05\n",
      " 8.55010148e-05 6.62361126e-05 5.97806466e-05 7.10111781e-05\n",
      " 8.79840154e-05 5.73620018e-05 8.37754633e-05 7.00709788e-05\n",
      " 1.17505610e-04 9.21775718e-05 7.58240349e-05 5.30928846e-05\n",
      " 7.92767096e-05 6.86460990e-05 8.52496814e-05 1.38975462e-04\n",
      " 1.05187661e-04 8.62102024e-05 5.55327460e-05 6.52415911e-05\n",
      " 6.52549134e-05 6.23741871e-05 1.22312240e-05 6.75927658e-05\n",
      " 1.00301477e-04 7.80129121e-05 6.11273717e-05 8.59291540e-05\n",
      " 4.71268831e-05 6.37939738e-05 8.23293740e-05 5.30029902e-05\n",
      " 9.03744003e-05 4.36037662e-05 1.17456650e-04 9.35327334e-05\n",
      " 1.12944901e-04 8.14253144e-05 7.82774805e-05 6.07887123e-05\n",
      " 4.84117954e-05 7.62587515e-05 6.21292638e-05 4.06184590e-05\n",
      " 7.13337577e-05 8.85906557e-05 7.29177264e-05 8.32160731e-05\n",
      " 7.80839619e-05 8.94350815e-05 6.29252609e-05 7.23071425e-05\n",
      " 6.05118330e-05 5.05920383e-04 6.06596623e-05 2.20661590e-04\n",
      " 5.97207545e-05 8.08507029e-05 8.10093188e-05 7.74159344e-05\n",
      " 5.45601724e-05 6.81437596e-05 6.69088040e-05 6.36938785e-05\n",
      " 6.40743237e-05 1.07042906e-04 5.65968294e-05 5.47367417e-05\n",
      " 8.24157396e-05 1.11063171e-04 1.00151316e-04 1.04903585e-04\n",
      " 9.66876687e-05 6.49452340e-05 4.98734407e-05 3.88330336e-05\n",
      " 7.17127987e-05 8.54862155e-05 7.94891748e-05 5.48884163e-05\n",
      " 6.80252342e-05 7.38839371e-05 7.78677204e-05 7.08829684e-05\n",
      " 6.27701447e-05 6.62074381e-05 6.48585192e-05 7.27994047e-05\n",
      " 1.00364683e-04 5.85774142e-05 5.66317576e-05 9.92851128e-05\n",
      " 5.93324839e-05 7.70914339e-05 7.76436136e-05 5.54947328e-05\n",
      " 5.89842202e-05 5.87259201e-05 7.69600956e-05 7.64543001e-05\n",
      " 5.56390878e-05 3.06753536e-05 7.86334786e-05 4.69501647e-05\n",
      " 5.96856262e-05 8.94335026e-05 7.86340752e-05 1.10113870e-04\n",
      " 7.20789394e-05 9.23341358e-05 8.21350404e-05 6.50365619e-05\n",
      " 8.19602428e-05 5.08979647e-05 1.03918173e-04 1.13629525e-04\n",
      " 4.85690034e-05 6.27523405e-05 4.22772937e-05 5.99278610e-05\n",
      " 6.90207307e-05 6.91374662e-05 8.16097163e-05 1.05206862e-04\n",
      " 7.08274165e-05 6.51983064e-05 9.15436176e-05 6.58510617e-05\n",
      " 9.83378413e-05 7.81197887e-05 8.88461364e-05 4.61795316e-05\n",
      " 7.63405696e-05 1.03137158e-04 6.16257821e-05 6.76547425e-05\n",
      " 1.07228130e-04 9.63213388e-05 1.04477433e-04 7.10074528e-05\n",
      " 5.04647032e-05 7.54142020e-05 6.79632431e-05 8.69792057e-05\n",
      " 1.02620790e-04 6.40339713e-05 9.68356035e-05 8.42598965e-05\n",
      " 8.96269048e-05 6.84967381e-05 5.88811163e-05 8.69763462e-05\n",
      " 7.14296693e-05 5.59046020e-05 5.73874713e-05 7.08503649e-05\n",
      " 5.35346953e-05 7.77188907e-05 5.61439119e-05 7.44912250e-05\n",
      " 7.50417530e-05 7.59178147e-05 6.45729524e-05 8.13624429e-05\n",
      " 5.53040882e-05 6.70779191e-05 1.10135617e-04 5.50081822e-05\n",
      " 8.86408525e-05 5.42594316e-05 7.84344738e-05 5.01304421e-05\n",
      " 8.71275261e-05 5.30011494e-05 6.21650324e-05 8.60455111e-05\n",
      " 6.59202342e-05 7.53957938e-05 6.29532296e-05 2.62889025e-05\n",
      " 9.17322614e-05 8.16949469e-05 1.46515551e-04 9.11706375e-05\n",
      " 7.98935289e-05 9.06647183e-05 7.75703884e-05 8.16611500e-05\n",
      " 8.55003600e-05 3.85038693e-05 5.96098798e-05 6.78118377e-05\n",
      " 6.83989565e-05 5.65261944e-05 7.43569399e-05 3.27410999e-05\n",
      " 6.28981434e-05 7.51830812e-05 8.06295648e-05 7.84617077e-05\n",
      " 9.68493623e-05 9.65700456e-05 5.07648656e-05 5.97418657e-05\n",
      " 7.02624675e-05 7.02588804e-05 9.94167785e-05 8.41944711e-05\n",
      " 4.68821127e-05 7.35340436e-05 8.69758078e-05 6.77187709e-05\n",
      " 6.04192792e-05 7.19987002e-05 8.11681093e-05 7.73142601e-05\n",
      " 7.19957388e-05 8.87699425e-05 5.41323607e-05 6.80108933e-05\n",
      " 5.40284891e-05 5.54839971e-05 6.53349853e-05 5.60292647e-05\n",
      " 1.23749196e-04 5.08841731e-05 5.70853190e-05 5.31790902e-05\n",
      " 7.30975080e-05 1.73512290e-05 7.15462957e-05 6.96650022e-05\n",
      " 6.34590760e-05 7.98032561e-05 7.21972683e-05 7.30272368e-05\n",
      " 8.62438756e-05 8.38328342e-05 8.53478559e-05 7.58764800e-05\n",
      " 7.41172844e-05 5.82002103e-05 8.45040122e-05 6.25519679e-05\n",
      " 5.35730796e-05 8.86563721e-05 8.60213477e-05 8.53425590e-05\n",
      " 7.47519371e-05 7.61439660e-05 6.54679388e-05 6.30941431e-05\n",
      " 8.24621748e-05 7.23047560e-05 6.33356904e-05 9.78026801e-05\n",
      " 1.32860820e-04 8.04776137e-05 7.20713360e-05 9.07470239e-05\n",
      " 6.40629587e-05 7.67641177e-05 5.78058934e-05 9.21107421e-05\n",
      " 6.98923250e-05 7.64609067e-05 7.79817128e-05 5.62669265e-05\n",
      " 8.70700824e-05 7.96646709e-05 7.93519721e-05 9.60789475e-05\n",
      " 6.74715848e-05 9.38212397e-05 1.07729204e-04 4.88736005e-05\n",
      " 6.64521795e-05 4.81958414e-05 3.23161257e-05 5.66457493e-05\n",
      " 1.71802909e-04 5.57532076e-05 6.96474672e-05 9.80058539e-05\n",
      " 4.85900164e-05 5.64477050e-05 1.59055460e-04 1.10823363e-04\n",
      " 8.39900968e-05 9.03697000e-05 6.60649894e-05 5.55543847e-05\n",
      " 8.77100611e-05 3.82054895e-05 7.53451968e-05 6.20902283e-05\n",
      " 6.91045061e-05 6.10041752e-05 8.61139779e-05 1.03400205e-04\n",
      " 1.03725943e-04 1.13056216e-04 7.58831011e-05 9.22291511e-05\n",
      " 5.67593634e-05 7.73584688e-05 2.03708565e-04 6.25082830e-05\n",
      " 7.18271403e-05 6.39714708e-05 8.04734955e-05 7.86590899e-05\n",
      " 6.02467262e-05 5.37895758e-05 6.16918769e-05 7.02470206e-05\n",
      " 7.49936007e-05 3.05761496e-05 5.65010014e-05 8.15585227e-05\n",
      " 8.30267381e-05 5.57160529e-05 8.28523625e-05 6.71187081e-05\n",
      " 4.86092031e-05 7.68285681e-05 1.00572608e-04 7.40606483e-05\n",
      " 8.95045450e-05 7.55143847e-05 8.30660938e-05 8.16679531e-05\n",
      " 1.01011741e-04 6.66394262e-05 8.24458475e-05 6.79388686e-05\n",
      " 7.15546557e-05 7.39604220e-05 5.68725845e-05 7.14469497e-05\n",
      " 5.52426282e-05 5.95796773e-05 6.63974788e-05 8.91409727e-05\n",
      " 3.44372456e-05 5.35494764e-05 6.93184920e-05 5.17957415e-05\n",
      " 8.80420630e-05 4.06883446e-05 7.14001872e-05 7.90712438e-05\n",
      " 9.09871524e-05 8.67614945e-05 7.49784085e-05 7.18186420e-05\n",
      " 6.59820362e-05 7.62995987e-05 7.17467337e-05 5.92816978e-05\n",
      " 6.12119329e-05 5.52028687e-05 1.19338300e-04 7.41814947e-05\n",
      " 9.11363095e-05 1.08710374e-04 8.63134410e-05 2.98253162e-05\n",
      " 7.20566313e-05 6.30795548e-05 6.02771834e-05 7.92267456e-05\n",
      " 9.77259042e-05 6.75690462e-05 9.22035979e-05 1.13492366e-04]\n",
      "Positive [6.33858945e-05 5.92390861e-05 9.81002522e-05 5.87245449e-05\n",
      " 6.34185781e-05 1.05980507e-04 7.45595607e-05 8.76981503e-05\n",
      " 1.05202198e-04 7.56161753e-05 7.36191287e-05 1.09512861e-04\n",
      " 7.34135974e-05 6.19358761e-05 5.78209110e-05 6.52580784e-05\n",
      " 5.67445604e-05 9.70947513e-05 1.26264378e-04 8.03143776e-05\n",
      " 7.03402402e-05 1.55073241e-04 7.84965887e-05 4.48930114e-05\n",
      " 9.21783285e-05 5.57694839e-05 6.16618199e-05 8.44685492e-05\n",
      " 8.82793247e-05 6.70472436e-05 7.94920416e-05 8.02725262e-05\n",
      " 8.28385819e-05 6.68960347e-05 7.66523808e-05 5.57313069e-05\n",
      " 6.50286238e-05 4.43726822e-05 6.87695501e-05 1.05430088e-04\n",
      " 5.03204101e-05 6.48689675e-05 7.72871281e-05 9.24404885e-05\n",
      " 6.90154266e-05 9.78685130e-05 7.49160754e-05 5.97117032e-05\n",
      " 8.62934394e-05 6.62568636e-05 4.18597374e-05 7.70859915e-05\n",
      " 1.21428129e-04 5.20078429e-05 5.28361670e-05 7.75128210e-05\n",
      " 5.59287328e-05 1.13236463e-04 1.06759930e-04 6.86044805e-05\n",
      " 8.33018712e-05 7.34378991e-05 4.61538839e-05 7.20174794e-05\n",
      " 7.71269479e-05 5.56639752e-05 1.10917412e-04 5.43482674e-05\n",
      " 7.78500471e-05 9.09588634e-05 1.07812863e-04 9.88048123e-05\n",
      " 7.52595879e-05 4.69419756e-05 6.98683943e-05 8.69625073e-05\n",
      " 8.83229004e-05 8.90043339e-06 1.01198311e-04 9.32456314e-05\n",
      " 8.40084758e-05 9.50377798e-05 6.36854093e-05 4.71205058e-05\n",
      " 8.57120613e-05 6.21067011e-05 9.51331313e-05 8.23045193e-05\n",
      " 8.62660745e-05 8.87388014e-05 5.77442661e-05 1.05790161e-04\n",
      " 4.76075002e-05 4.52365894e-05 5.06951765e-05 8.81884116e-05\n",
      " 6.96234565e-05 1.08823799e-04 1.10788649e-04 1.00475387e-04\n",
      " 7.49076789e-05 6.44752072e-05 6.08775117e-05 6.39470454e-05\n",
      " 8.91905875e-05 5.31696787e-05 7.01386816e-05 5.28033233e-05\n",
      " 8.24102535e-05 7.61261763e-05 5.38220083e-05 4.83620097e-05\n",
      " 6.72748210e-05 8.38593478e-05 6.39753125e-05 6.83670296e-05\n",
      " 6.93945913e-05 7.55046276e-05 5.47708150e-05 8.41075162e-05\n",
      " 8.01918795e-05 1.92118401e-04 9.73278566e-06 7.50809777e-05\n",
      " 7.54443754e-05 8.33121157e-05 6.80646845e-05 8.55304606e-05\n",
      " 7.75574517e-05 3.62990140e-05 8.90338415e-05 6.30022332e-05\n",
      " 7.62854834e-05 6.06472822e-05 7.89556289e-05 7.07465151e-05\n",
      " 1.15524657e-04 1.01261503e-04 5.10676255e-05 6.09255112e-05\n",
      " 7.97710964e-05 6.80891899e-05 6.28991329e-05 8.74786274e-05\n",
      " 6.79959776e-05 7.47243830e-05 5.01522481e-05 8.78352512e-05\n",
      " 6.23986634e-05 7.04379418e-05 9.16373465e-05 1.12134687e-04\n",
      " 8.19059715e-05 7.26564904e-05 9.01531312e-05 5.25060459e-05\n",
      " 9.15915225e-05 9.15589917e-05 6.99983866e-05 7.40744217e-05\n",
      " 1.25848543e-04 6.80525773e-05 8.54143800e-05 4.15334871e-05\n",
      " 8.23350274e-05 8.20252098e-05 8.94383702e-05 7.67748861e-05\n",
      " 8.75916230e-05 4.15123795e-05 7.71695559e-05 1.45115293e-04\n",
      " 6.52895251e-05 7.68430400e-05 7.38038289e-05 5.17392436e-05\n",
      " 8.36143081e-05 9.48698071e-05 8.44178649e-05 6.25759567e-05\n",
      " 8.80012230e-05 7.86149540e-05 6.71940215e-05 8.77757921e-05\n",
      " 9.96419112e-05 8.76450576e-05 2.24273579e-04 5.80505694e-05\n",
      " 7.47698650e-05 9.71877671e-05 8.27000404e-05 6.07908587e-05\n",
      " 8.44215247e-05 1.10724228e-04 8.30942226e-05 8.85154150e-05\n",
      " 3.68067886e-05 8.38623891e-05 5.62551540e-05 9.15688506e-05\n",
      " 6.62851744e-05 6.36919067e-05 5.31565202e-05 1.28707819e-04\n",
      " 7.44981808e-05 6.70324735e-05 5.49361794e-05 6.39417340e-05\n",
      " 8.48780546e-05 7.26264188e-05 6.50705551e-05 6.57759956e-05\n",
      " 7.09820015e-05 1.11303874e-04 4.21692639e-05 5.70845883e-03\n",
      " 6.15509489e-05 6.72177121e-05 1.03047299e-04 9.55838259e-05\n",
      " 5.44255963e-05 8.56921106e-05 9.25024433e-05 6.31373696e-05\n",
      " 8.46085240e-05 5.71875389e-05 9.48728339e-05 8.87659335e-05\n",
      " 5.20959293e-05 7.39449533e-05 1.15955510e-04 4.19428725e-05\n",
      " 9.28060836e-05 7.67712554e-05 7.67143501e-05 7.56231020e-05\n",
      " 6.69516594e-05 4.57006754e-05 1.46168037e-04 5.65104019e-05\n",
      " 1.08452376e-04 8.85392656e-05 6.54337637e-05 7.40078685e-05\n",
      " 8.18599801e-05 8.26981195e-05 5.99805717e-05 5.82695757e-05\n",
      " 8.11867721e-05 6.35631732e-05 8.33546073e-05 6.70465379e-05\n",
      " 1.01113423e-04 5.88498478e-05 7.40004616e-05 9.18074875e-05\n",
      " 8.81317392e-05 7.57032685e-05 5.25188480e-05 5.28245073e-05\n",
      " 7.06171559e-05 9.38460798e-05 4.59562616e-05 6.87477732e-05\n",
      " 7.10565000e-05 8.95564081e-05 4.55892332e-05 6.48101704e-05\n",
      " 6.88654982e-05 9.90298504e-05 5.29857825e-05 5.36070656e-05\n",
      " 8.22935763e-05 3.89884808e-05 5.30511534e-05 6.92996618e-05\n",
      " 9.29197558e-05 7.29231615e-05 1.03345388e-04 8.12555518e-05\n",
      " 1.48938678e-04 6.24469831e-05 3.29778850e-05 6.46055923e-05\n",
      " 7.74657528e-05 6.40880098e-05 6.65425177e-05 7.62923883e-05\n",
      " 8.89689545e-05 6.85359992e-05 3.73046933e-05 8.81368978e-05\n",
      " 6.36204786e-05 6.96337811e-05 8.59974898e-05 5.96227910e-05\n",
      " 7.50287218e-05 4.82823998e-05 7.95560991e-05 6.57231940e-05\n",
      " 1.17905794e-04 6.80805242e-05 6.34779644e-05 8.15875464e-05\n",
      " 6.82230238e-05 8.15107851e-05 6.48098285e-05 5.45889307e-05\n",
      " 4.92365398e-05 5.50348377e-05 6.60092846e-05 6.00036292e-05\n",
      " 1.03154904e-04 4.01892721e-05 9.41915569e-05 9.03241234e-05\n",
      " 6.70063382e-05 7.83723808e-05 9.82816782e-05 9.97951720e-05\n",
      " 6.56504999e-05 5.80434535e-05 1.02689657e-04 6.63241954e-05\n",
      " 6.30863869e-05 7.93152649e-05 6.93348265e-05 6.34741227e-05\n",
      " 7.94901935e-05 1.06449952e-04 7.64415818e-05 6.72382594e-05\n",
      " 1.04911102e-04 9.97124152e-05 8.26334654e-05 6.03323970e-05\n",
      " 1.05350591e-04 7.32273838e-05 6.69058354e-05 7.79731272e-05\n",
      " 1.75413166e-04 5.82259709e-05 4.84579650e-05 2.60183460e-05\n",
      " 9.99564727e-05 9.41714388e-05 5.96020363e-05 6.37790363e-05\n",
      " 7.66890880e-05 5.36964799e-05 5.19163877e-05 6.50960064e-05\n",
      " 7.89599508e-05 7.05995844e-05 6.77105418e-05 1.00881778e-04\n",
      " 9.79100005e-05 6.07322763e-05 6.90973247e-05 7.74875007e-05\n",
      " 7.43302735e-05 5.20583053e-05 7.87897588e-05 6.80616940e-05\n",
      " 6.59418729e-05 9.84994258e-05 7.88207617e-05 1.17436321e-04\n",
      " 7.83474316e-05 6.21933141e-05 7.35359718e-05 7.37733571e-05\n",
      " 1.33001042e-04 6.73051982e-05 5.48584831e-05 9.50421818e-05\n",
      " 5.94989942e-05 6.61002879e-04 5.85360904e-05 6.82607715e-05\n",
      " 6.77283388e-05 8.51213044e-05 9.69904504e-05 5.91619355e-05\n",
      " 4.97014153e-05 6.34448370e-05 6.03193366e-05 7.75870358e-05\n",
      " 1.24169572e-04 8.85692061e-05 6.41691004e-05 7.83216456e-05\n",
      " 8.82006789e-05 9.31123432e-05 7.79062684e-05 7.63350763e-05\n",
      " 1.47275117e-04 4.78713082e-05 7.48110251e-05 6.84432889e-05\n",
      " 1.16616851e-04 8.12062062e-05 1.07859145e-04 7.33137931e-05\n",
      " 8.29570126e-05 1.00780133e-04 9.28513764e-05 8.90244119e-05\n",
      " 8.33603553e-05 5.09471101e-05 1.11966750e-04 7.36999064e-05\n",
      " 7.43037672e-05 5.89680458e-05 1.11225390e-05 5.90743475e-05\n",
      " 7.99539775e-05 7.74377550e-05 6.47056731e-05 9.36562647e-05\n",
      " 6.40012731e-05 7.79197071e-05 5.19984969e-05 7.07851505e-05\n",
      " 6.95527415e-05 3.66285713e-05 1.03215876e-04 8.09303529e-05\n",
      " 7.09623346e-05 6.00103303e-05 8.20816349e-05 5.38732347e-05\n",
      " 7.35892318e-05 6.47749475e-05 8.84340407e-05 7.27424995e-05\n",
      " 6.01387146e-05 5.56810228e-05 8.29232013e-05 7.78602553e-05\n",
      " 6.51498558e-05 8.58902422e-05 7.54334105e-05 7.21205724e-05\n",
      " 6.31708826e-05 4.15219605e-04 3.71479364e-05 7.19734380e-05\n",
      " 6.42847808e-05 8.35879182e-05 7.22141704e-05 5.51930789e-05\n",
      " 6.47298657e-05 6.61138401e-05 9.05123234e-05 7.66123630e-05\n",
      " 8.91388918e-05 7.09687301e-05 6.40972139e-05 5.76377715e-05\n",
      " 6.44769607e-05 7.42647971e-05 7.31089822e-05 1.52294902e-04\n",
      " 8.11009813e-05 7.37838855e-05 3.27728885e-05 4.73281943e-05\n",
      " 6.58957288e-05 5.64397087e-05 8.04784504e-05 5.88403091e-05\n",
      " 7.43952696e-05 9.92200949e-05 9.44328640e-05 7.52704218e-05\n",
      " 5.43884526e-05 8.32683072e-05 7.41165859e-05 6.76296477e-05\n",
      " 5.26344447e-05 7.47492231e-05 6.36880504e-05 8.77124476e-05\n",
      " 5.96015889e-05 7.00213932e-05 7.05140264e-05 1.31567678e-04\n",
      " 4.79074588e-05 6.80054436e-05 7.64688448e-05 5.74134210e-05\n",
      " 7.21755132e-05 6.84983024e-05 6.39845312e-05 5.54547951e-05\n",
      " 5.97718317e-05 8.74351317e-05 3.32481359e-05 9.24600608e-05\n",
      " 6.79029545e-05 9.28531445e-05 5.78044564e-05 6.32000811e-05\n",
      " 8.67249255e-05 5.42275411e-05 9.64466162e-05 6.22146472e-05\n",
      " 5.00537244e-05 6.55747208e-05 7.49721876e-05 6.60209698e-05\n",
      " 6.25766697e-05 4.88333644e-05 6.42145242e-05 3.62039573e-05\n",
      " 7.94114312e-05 5.77150313e-05 7.23260673e-05 6.45035543e-05\n",
      " 8.08665500e-05 6.39706122e-05 6.53617608e-05 4.61693380e-05\n",
      " 8.67821363e-05 7.24254642e-05 4.96501598e-05 7.22331170e-05\n",
      " 7.61039628e-05 6.27310656e-05 7.62141426e-05 7.23362100e-05\n",
      " 9.91413035e-05 1.39531185e-04 8.41468718e-05 8.68921998e-05\n",
      " 9.63783095e-05 8.04265073e-05 9.23731204e-05 4.81365823e-05\n",
      " 8.03574294e-05 6.94348491e-05 1.09071967e-04 4.44740799e-05\n",
      " 8.71380835e-05 6.71094240e-05 7.02730540e-05 8.00500638e-05\n",
      " 7.00099990e-05 8.74894758e-05 4.33630848e-05 5.63734066e-05\n",
      " 8.42094087e-05 7.16691866e-05 1.08390755e-04 8.08503173e-05\n",
      " 1.08162661e-04 6.45527180e-05 1.01358542e-04 7.43249257e-05\n",
      " 5.13886043e-05 8.97171631e-05 1.11314053e-04 6.25105167e-05\n",
      " 6.05621462e-05 6.04035849e-05 1.11576781e-04 7.97882967e-05\n",
      " 7.26738144e-05 7.45783327e-05 8.87455244e-05 6.21928120e-05\n",
      " 7.65565565e-05 5.92050565e-05 7.54605644e-05 7.67719830e-05\n",
      " 9.65272266e-05 9.26449793e-05 7.79055263e-05 5.93924997e-05\n",
      " 1.06752908e-04 8.29847049e-05 6.92435715e-05 8.34074381e-05\n",
      " 9.73120477e-05 4.28201638e-05 9.20958628e-05 5.10013342e-05\n",
      " 6.86060739e-05 1.32120782e-04 6.82762620e-05 7.19573072e-05\n",
      " 7.31279433e-05 5.98510669e-05 5.25203504e-05 5.56328596e-05\n",
      " 6.77674470e-05 6.31806470e-05 8.04274314e-05 7.45746802e-05\n",
      " 6.84978804e-05 1.48053223e-04 7.12876863e-05 8.95503836e-05\n",
      " 7.21338511e-05 5.83511210e-05 1.04472238e-04 7.79897746e-05\n",
      " 4.79240007e-05 9.99131589e-05 6.47339984e-05 9.30065507e-05\n",
      " 6.09141280e-05 8.14398736e-05 1.07179723e-04 3.95885581e-05\n",
      " 1.04819097e-04 5.58854954e-05 8.25887182e-05 7.12457550e-05\n",
      " 7.82299467e-05 4.57596616e-05 1.11956084e-04 7.42955090e-05\n",
      " 1.08166838e-04 5.84512964e-05 7.21437536e-05 4.60462797e-05\n",
      " 6.94931077e-05 8.22460643e-05 1.46343780e-04 6.97147188e-05\n",
      " 6.24725362e-05 9.74921131e-05 6.44979591e-05 6.08412884e-05\n",
      " 7.71933919e-05 1.05463223e-04 1.01801510e-04 7.17390358e-05\n",
      " 9.58781748e-05 7.34156565e-05 5.55700462e-05 4.95956665e-05\n",
      " 5.95562960e-05 5.35002000e-05 5.31575097e-05 8.59744832e-05\n",
      " 2.83986621e-04 1.06052634e-04 8.96977508e-05 6.42204104e-05\n",
      " 5.56276318e-05 7.18965312e-05 3.91198373e-05 9.04272092e-05\n",
      " 8.76820122e-05 7.06925930e-05 9.00006416e-05 6.76862983e-05\n",
      " 7.29439562e-05 6.90725137e-05 1.60424286e-04 5.67033676e-05\n",
      " 8.17106193e-05 1.23254795e-04 9.34695636e-05 6.35531687e-05\n",
      " 8.17763794e-05 1.08218781e-04 6.92118847e-05 7.87942772e-05\n",
      " 7.19640957e-05 8.26278047e-05 6.48247296e-05 6.69807487e-05\n",
      " 6.23460219e-05 2.18704172e-05 7.32214612e-05 9.63307975e-05\n",
      " 7.55388028e-05 7.97569955e-05 7.17227522e-05 4.69278784e-05\n",
      " 8.72518867e-05 7.06357823e-05 9.04914777e-05 7.47384984e-05\n",
      " 6.61369195e-05 8.37288972e-05 8.11157224e-05 7.46264268e-05\n",
      " 9.85935476e-05 6.74616167e-05 5.69414769e-05 6.70036898e-05\n",
      " 7.73489228e-05 7.56215813e-05 1.07480912e-04 7.67057572e-05\n",
      " 8.58517378e-05 5.60032458e-05 9.02187894e-05 8.97259742e-05\n",
      " 9.33717747e-05 8.87390997e-05 4.58268696e-05 7.89917030e-05\n",
      " 7.72094500e-05 1.58187453e-04 5.83532383e-05 9.35625358e-05\n",
      " 9.46571163e-05 8.31597936e-05 4.75172819e-05 8.69925643e-05\n",
      " 5.09495658e-05 7.97701505e-05 7.18670854e-05 1.15111914e-04\n",
      " 9.58892342e-05 7.20677344e-05 7.95771193e-05 4.48623905e-05\n",
      " 4.91259634e-05 7.84702715e-05 8.62030865e-05 5.23033195e-05\n",
      " 8.48869531e-05 6.87361680e-05 6.29891729e-05 7.09993692e-05\n",
      " 7.99448098e-05 5.99580126e-05 7.22694604e-05 7.85407246e-05\n",
      " 1.04744198e-04 6.90674206e-05 5.55064871e-05 5.18008092e-05\n",
      " 5.41789668e-05 6.80894154e-05 1.23898441e-04 8.43610833e-05\n",
      " 5.51968624e-05 7.34084533e-05 5.75455233e-05 6.60526421e-05\n",
      " 6.99834345e-05 6.73778050e-05 7.06965366e-05 7.60804905e-05\n",
      " 5.38110507e-05 6.83757607e-05 1.13879811e-04 1.02178798e-04\n",
      " 6.07617949e-05 8.30352146e-05 7.52592314e-05 6.02593973e-05\n",
      " 8.50062788e-05 9.84733124e-05 9.99085387e-05 7.70578336e-05\n",
      " 8.63456298e-05 7.23688863e-05 9.58892342e-05 6.82562459e-05]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[85], line 12\u001b[0m\n\u001b[1;32m      8\u001b[0m scores \u001b[38;5;241m=\u001b[39m softmax(scores)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(scores)):\n\u001b[0;32m---> 12\u001b[0m     l \u001b[38;5;241m=\u001b[39m \u001b[43mlabels\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     13\u001b[0m     s \u001b[38;5;241m=\u001b[39m scores[i]\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28mprint\u001b[39m(l,s)\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "labels = ['Negative', 'Neutral', 'Positive']\n",
    "\n",
    "# sentiment analysis\n",
    "encoded_tweet = tokenizer(text, return_tensors='pt')\n",
    "output = bertweet(**encoded_tweet)\n",
    "\n",
    "scores = output[0][0].detach().numpy()\n",
    "scores = softmax(scores)\n",
    "\n",
    "for i in range(len(scores)):\n",
    "    \n",
    "    l = labels[i]\n",
    "    s = scores[i]\n",
    "    print(l,s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.functional import softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.0506, 0.0742, 0.0666,  ..., 0.0662, 0.0692, 0.0705],\n",
      "         [0.0818, 0.0678, 0.0852,  ..., 0.0678, 0.0899, 0.1230],\n",
      "         [0.0415, 0.0570, 0.0775,  ..., 0.0726, 0.0935, 0.0740],\n",
      "         ...,\n",
      "         [0.0903, 0.0535, 0.0661,  ..., 0.0586, 0.0755, 0.0698],\n",
      "         [0.0831, 0.0851, 0.0766,  ..., 0.0790, 0.0978, 0.0665],\n",
      "         [0.0502, 0.0751, 0.0683,  ..., 0.0639, 0.0715, 0.0717]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "probs = softmax(logits, dim=1)\n",
    "print(probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0506, 0.0742, 0.0666,  ..., 0.0662, 0.0692, 0.0705],\n",
      "        [0.0818, 0.0678, 0.0852,  ..., 0.0678, 0.0899, 0.1230],\n",
      "        [0.0415, 0.0570, 0.0775,  ..., 0.0726, 0.0935, 0.0740],\n",
      "        ...,\n",
      "        [0.0903, 0.0535, 0.0661,  ..., 0.0586, 0.0755, 0.0698],\n",
      "        [0.0831, 0.0851, 0.0766,  ..., 0.0790, 0.0978, 0.0665],\n",
      "        [0.0502, 0.0751, 0.0683,  ..., 0.0639, 0.0715, 0.0717]],\n",
      "       grad_fn=<MeanBackward1>)\n"
     ]
    }
   ],
   "source": [
    "average_probs = torch.mean(probs, dim=0)\n",
    "print(average_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La classe prédite est : 445\n"
     ]
    }
   ],
   "source": [
    "predicted_class = torch.argmax(average_probs).item()\n",
    "\n",
    "# Afficher la classe prédite\n",
    "print(f\"La classe prédite est : {predicted_class}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'I am so happy to see you at this evening!'\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "outputs = bertweet(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenizer.encode(text, add_special_tokens=True, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.functional import softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'BaseModelOutputWithPoolingAndCrossAttentions' object has no attribute 'logits'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[64], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m probs \u001b[38;5;241m=\u001b[39m softmax(\u001b[43moutputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlogits\u001b[49m, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(probs)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'BaseModelOutputWithPoolingAndCrossAttentions' object has no attribute 'logits'"
     ]
    }
   ],
   "source": [
    "probs = softmax(outputs.logits, dim=1)\n",
    "print(probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized = batch_1['words'].apply((lambda x: tokenizer.encode(x, add_special_tokens=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 0\n",
    "for i in tokenized.values:\n",
    "    if len(i) > max_len:\n",
    "        max_len = len(i)\n",
    "\n",
    "padded = np.array([i + [0]*(max_len-len(i)) for i in tokenized.values])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 37)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_mask = np.where(padded != 0, 1, 0)\n",
    "attention_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = torch.tensor(padded)  \n",
    "attention_mask = torch.tensor(attention_mask)\n",
    "\n",
    "with torch.no_grad():\n",
    "    last_hidden_states = bertweet(input_ids, attention_mask=attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = last_hidden_states[0][:,0,:].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = batch_1['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_features, test_features, train_labels, test_labels = train_test_split(features, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gaeldelescluse/.local/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr_clf = LogisticRegression()\n",
    "lr_clf.fit(train_features, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_clf.score(test_features, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "emoji is not installed, thus not converting emoticons or emojis into text. Install emoji: pip3 install emoji==0.6.0\n"
     ]
    }
   ],
   "source": [
    "bertweet = AutoModel.from_pretrained(\"vinai/bertweet-base\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"vinai/bertweet-base\", use_fast=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "emoji is not installed, thus not converting emoticons or emojis into text. Install emoji: pip3 install emoji==0.6.0\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "# Charger le modèle BERTweet et le tokenizer\n",
    "bertweet = AutoModel.from_pretrained(\"vinai/bertweet-base\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"vinai/bertweet-base\", use_fast=False)\n",
    "\n",
    "# Texte d'exemple\n",
    "text = \"It was an excellent movie, I loved every moment!\"\n",
    "\n",
    "# Tokenisation et encodage avec le tokenizer BERTweet\n",
    "tokens = tokenizer.encode(text, add_special_tokens=True, return_tensors=\"pt\")\n",
    "\n",
    "# Utilisation du modèle pour obtenir les logits\n",
    "outputs = bertweet(tokens)\n",
    "logits = outputs.last_hidden_state  # Si vous souhaitez obtenir les embeddings finaux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative 0.0020959747\n",
      "Neutral 0.01897672\n",
      "Positive 0.97892725\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from scipy.special import softmax\n",
    "\n",
    "# tweet = \"@MehranShakarami today's cold @ home 😒 https://mehranshakarami.com\"\n",
    "tweet = 'Great content! subscribed 😉'\n",
    "\n",
    "# precprcess tweet\n",
    "tweet_words = []\n",
    "\n",
    "for word in tweet.split(' '):\n",
    "    if word.startswith('@') and len(word) > 1:\n",
    "        word = '@user'\n",
    "    \n",
    "    elif word.startswith('http'):\n",
    "        word = \"http\"\n",
    "    tweet_words.append(word)\n",
    "\n",
    "tweet_proc = \" \".join(tweet_words)\n",
    "\n",
    "# load model and tokenizer\n",
    "roberta = \"cardiffnlp/twitter-roberta-base-sentiment\"\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(roberta)\n",
    "tokenizer = AutoTokenizer.from_pretrained(roberta)\n",
    "\n",
    "labels = ['Negative', 'Neutral', 'Positive']\n",
    "\n",
    "# sentiment analysis\n",
    "encoded_tweet = tokenizer(tweet_proc, return_tensors='pt')\n",
    "output = model(**encoded_tweet)\n",
    "\n",
    "scores = output[0][0].detach().numpy()\n",
    "scores = softmax(scores)\n",
    "\n",
    "for i in range(len(scores)):\n",
    "    \n",
    "    l = labels[i]\n",
    "    s = scores[i]\n",
    "    print(l,s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative 0.0020959747\n",
      "Neutral 0.01897672\n",
      "Positive 0.97892725\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from scipy.special import softmax\n",
    "\n",
    "# tweet = \"@MehranShakarami today's cold @ home 😒 https://mehranshakarami.com\"\n",
    "tweet = 'Great content! subscribed 😉'\n",
    "\n",
    "# precprcess tweet\n",
    "tweet_words = []\n",
    "\n",
    "#for word in tweet.split(' '):\n",
    "#    if word.startswith('@') and len(word) > 1:\n",
    "#        word = '@user'\n",
    "#   \n",
    "#    elif word.startswith('http'):\n",
    "#        word = \"http\"\n",
    "#    tweet_words.append(word)\n",
    "\n",
    "\n",
    "#tweet_proc = \" \".join(tweet_words)\n",
    "tweet_proc = tweet\n",
    "\n",
    "# load model and tokenizer\n",
    "roberta = \"cardiffnlp/twitter-roberta-base-sentiment\"\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(roberta)\n",
    "tokenizer = AutoTokenizer.from_pretrained(roberta)\n",
    "\n",
    "labels = ['Negative', 'Neutral', 'Positive']\n",
    "\n",
    "# sentiment analysis\n",
    "encoded_tweet = tokenizer(tweet_proc, return_tensors='pt')\n",
    "output = model(**encoded_tweet)\n",
    "\n",
    "scores = output[0][0].detach().numpy()\n",
    "scores = softmax(scores)\n",
    "\n",
    "for i in range(len(scores)):\n",
    "    \n",
    "    l = labels[i]\n",
    "    s = scores[i]\n",
    "    print(l,s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative 0.0020959747\n",
      "Neutral 0.01897672\n",
      "Positive 0.97892725\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from scipy.special import softmax\n",
    "\n",
    "tweet = 'Great content! subscribed 😉'\n",
    "tweet_proc = tweet\n",
    "#roberta = \"cardiffnlp/twitter-roberta-base-sentiment\"\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(roberta)\n",
    "tokenizer = AutoTokenizer.from_pretrained(roberta)\n",
    "#bertweet = AutoModel.from_pretrained(\"vinai/bertweet-base\")\n",
    "#tokenizer = AutoTokenizer.from_pretrained(\"vinai/bertweet-base\", use_fast=False)\n",
    "\n",
    "labels = ['Negative', 'Neutral', 'Positive']\n",
    "\n",
    "# sentiment analysis\n",
    "encoded_tweet = tokenizer(tweet_proc, return_tensors='pt')\n",
    "output = model(**encoded_tweet)\n",
    "\n",
    "scores = output[0][0].detach().numpy()\n",
    "scores = softmax(scores)\n",
    "\n",
    "for i in range(len(scores)):\n",
    "    \n",
    "    l = labels[i]\n",
    "    s = scores[i]\n",
    "    print(l,s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00209597 0.01897672 0.97892725]\n"
     ]
    }
   ],
   "source": [
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from scipy.special import softmax\n",
    "\n",
    "tweet = 'Great content! subscribed 😉'\n",
    "tweet_proc = tweet\n",
    "#roberta = \"cardiffnlp/twitter-roberta-base-sentiment\"\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(roberta)\n",
    "tokenizer = AutoTokenizer.from_pretrained(roberta)\n",
    "#bertweet = AutoModel.from_pretrained(\"vinai/bertweet-base\")\n",
    "#tokenizer = AutoTokenizer.from_pretrained(\"vinai/bertweet-base\", use_fast=False)\n",
    "\n",
    "labels = ['Negative', 'Neutral', 'Positive']\n",
    "\n",
    "# sentiment analysis\n",
    "encoded_tweet = tokenizer(tweet_proc, return_tensors='pt')\n",
    "output = model(**encoded_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "softmax() got an unexpected keyword argument 'dim'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[97], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mspecial\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m softmax\n\u001b[1;32m      3\u001b[0m logits \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlast_hidden_state\n\u001b[0;32m----> 4\u001b[0m probs \u001b[38;5;241m=\u001b[39m \u001b[43msoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(probs)\n",
      "\u001b[0;31mTypeError\u001b[0m: softmax() got an unexpected keyword argument 'dim'"
     ]
    }
   ],
   "source": [
    "from scipy.special import softmax\n",
    "\n",
    "logits = outputs.last_hidden_state\n",
    "probs = softmax(logits, dim=1)\n",
    "print(probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/envs/Projet7bisEnv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment négatif\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "# Charger le modèle de tokenisation\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"nlptown/bert-base-multilingual-uncased-sentiment\")\n",
    "\n",
    "# Charger le modèle de classification de sentiment\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"nlptown/bert-base-multilingual-uncased-sentiment\")\n",
    "\n",
    "# Tokeniser la phrase\n",
    "text = \"Hello, my dog is cute\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "# Effectuer la prédiction\n",
    "outputs = model(**inputs)\n",
    "\n",
    "# Obtenir les probabilités de chaque classe\n",
    "probs = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "\n",
    "# Déterminer la classe prédite (positif ou négatif)\n",
    "predicted_class = torch.argmax(probs, dim=-1).item()\n",
    "\n",
    "# Afficher le résultat\n",
    "if predicted_class == 1:\n",
    "    print(\"Sentiment positif\")\n",
    "else:\n",
    "    print(\"Sentiment négatif\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Projet7Env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
