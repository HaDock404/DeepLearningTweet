{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Réalisez une analyse de sentiments grâce au Deep Learning  \n",
    "\n",
    "L'image de marque d'une entreprise est devenue intrinsèquement liée à sa présence et à sa réputation sur les réseaux sociaux. Les plateformes en ligne offrent une tribune sans précédent pour la diffusion d'opinions, qu'elles soient positives ou négatives.  \n",
    "La nécessité pour les entreprises de surveiller activement leur présence numérique et d'anticiper les bad buzz est devenue impérative.  \n",
    "\n",
    "![Air Paradis](https://user.oc-static.com/upload/2023/03/26/16798648691893_Ingenieur-IA-Graphics-P7-01-banner.png) \n",
    "\n",
    "Notre objectif est de fournir à [Air Paradis](https://google.com) un outil puissant et prédictif, capable d'analyser les tendances, de détecter les signaux faibles et de permettre une intervention proactive pour prévenir tout impact négatif sur sa réputation en ligne."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse exploratoire du jeu de données sur l'expression de sentiments de tweets.\n",
    "\n",
    "### Introduction :  \n",
    "\n",
    "\n",
    "L'objectif principal de notre projet est de développer un système intelligent capable d'analyser et de comprendre le sentiment d'un message posté par un utilisateur sur un réseau social, en l'occurence ici [Twitter](https://twitter.com/home?lang=fr). En utilisant des techniques avancées de traitement du langage naturel (NLP), notre solution cherchera à extraire des informations clés, à établir des liens sémantiques et à créer une représentation du sentiment de l'utilisateur."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nettoyage des données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import\n",
    "Nous utiliserons une stack de Data Science habituelle : `numpy`, `pandas`, `sklearn`, `matplotlib`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# manipulation des données\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# matplotlib et seaborn pour les représentations graphiques\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "#import missingno as msno\n",
    "\n",
    "# sklearn preprocessing pour le traiter les variables catégorielles\n",
    "#from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Gestion du système de fichiers\n",
    "import os\n",
    "\n",
    "# Suppression des alertes\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les données que nous utiliserons durant notre étude ont le chemin d'accès relatif suivant [\"../Data\"](). Nous pouvons retrouver dans le dossier [./Data]() un fichier csv qui comprend une liste de tweet d'utilisateur. C'est dans ce dossier que nous enregistrerons nos dataframes pour pouvoir les utiliser dans d'autres opérations de notre étude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2.sample_dataset.csv', '2.sample_dataset2.csv', '1.normalized_dataset.csv', '2.test_df.csv', 'training1600000.csv', '2.cleaned_dataset.csv', '2.train_val_df.csv']\n"
     ]
    }
   ],
   "source": [
    "print(os.listdir(\"../Data\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous importons le fichier qui nous sera utile pour notre étude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>flag</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599995</th>\n",
       "      <td>4</td>\n",
       "      <td>2193601966</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>AmandaMarie1028</td>\n",
       "      <td>Just woke up. Having no school is the best fee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599996</th>\n",
       "      <td>4</td>\n",
       "      <td>2193601969</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>TheWDBoards</td>\n",
       "      <td>TheWDB.com - Very cool to hear old Walt interv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599997</th>\n",
       "      <td>4</td>\n",
       "      <td>2193601991</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>bpbabe</td>\n",
       "      <td>Are you ready for your MoJo Makeover? Ask me f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599998</th>\n",
       "      <td>4</td>\n",
       "      <td>2193602064</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>tinydiamondz</td>\n",
       "      <td>Happy 38th Birthday to my boo of alll time!!! ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599999</th>\n",
       "      <td>4</td>\n",
       "      <td>2193602129</td>\n",
       "      <td>Tue Jun 16 08:40:50 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>RyanTrevMorris</td>\n",
       "      <td>happy #charitytuesday @theNSPCC @SparksCharity...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1600000 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         target          id                          date      flag  \\\n",
       "0             0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY   \n",
       "1             0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   \n",
       "2             0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY   \n",
       "3             0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "4             0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "...         ...         ...                           ...       ...   \n",
       "1599995       4  2193601966  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
       "1599996       4  2193601969  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
       "1599997       4  2193601991  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
       "1599998       4  2193602064  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
       "1599999       4  2193602129  Tue Jun 16 08:40:50 PDT 2009  NO_QUERY   \n",
       "\n",
       "                    user                                               text  \n",
       "0        _TheSpecialOne_  @switchfoot http://twitpic.com/2y1zl - Awww, t...  \n",
       "1          scotthamilton  is upset that he can't update his Facebook by ...  \n",
       "2               mattycus  @Kenichan I dived many times for the ball. Man...  \n",
       "3                ElleCTF    my whole body feels itchy and like its on fire   \n",
       "4                 Karoli  @nationwideclass no, it's not behaving at all....  \n",
       "...                  ...                                                ...  \n",
       "1599995  AmandaMarie1028  Just woke up. Having no school is the best fee...  \n",
       "1599996      TheWDBoards  TheWDB.com - Very cool to hear old Walt interv...  \n",
       "1599997           bpbabe  Are you ready for your MoJo Makeover? Ask me f...  \n",
       "1599998     tinydiamondz  Happy 38th Birthday to my boo of alll time!!! ...  \n",
       "1599999   RyanTrevMorris  happy #charitytuesday @theNSPCC @SparksCharity...  \n",
       "\n",
       "[1600000 rows x 6 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('../Data/training1600000.csv', encoding='latin-1', header=None)\n",
    "data = data.rename(columns={data.columns[0]: 'target'})\n",
    "data = data.rename(columns={data.columns[1]: 'id'})\n",
    "data = data.rename(columns={data.columns[2]: 'date'})\n",
    "data = data.rename(columns={data.columns[3]: 'flag'})\n",
    "data = data.rename(columns={data.columns[4]: 'user'})\n",
    "data = data.rename(columns={data.columns[5]: 'text'})\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le fichier contient 1.600.000 tweets d'utilisateurs extrait en utilisant l'API de Twitter. Les tweets ont été annoté (0 = negatif, 4 = positif) et peuvent être utilisé pour détecter le sentiment d'un tweet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le DataFrame est caractérisé par 6 variables :  \n",
    "\n",
    "1. target: le sentiment du tweet (0 = negatif, 4 = positif)\n",
    "2. id: identifiant du tweet ( 2087)\n",
    "3. date: date du tweet (Sat May 16 23:58:44 UTC 2009)\n",
    "4. flag: La requête (LyX). Si il n'y a pas de requête alors la valeur sera NO_QUERY\n",
    "5. user: l'utilisateur qui a tweeté (robotickilldozr)\n",
    "6. text: le texte du tweet (avecLyX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target                                                                                                                      0\n",
      "id                                                                                                                 1467810369\n",
      "date                                                                                             Mon Apr 06 22:19:45 PDT 2009\n",
      "flag                                                                                                                 NO_QUERY\n",
      "user                                                                                                          _TheSpecialOne_\n",
      "text      @switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "with pd.option_context('display.max_colwidth', None):\n",
    "    print(data.iloc[0, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "data['text'] = data['text'].apply(lambda x: re.sub(r'\\S*@\\S*\\s?', '', x))\n",
    "data['text'] = data['text'].apply(lambda x: re.sub(r'http\\S+', '', x))\n",
    "data['text'] = data['text'].apply(lambda x: re.sub(r'[^\\w\\s]', '', x))\n",
    "data['text'] = data['text'].apply(lambda x: re.sub(r'\\b\\w\\b', '', x))\n",
    "data['text'] = data['text'].apply(lambda x: re.sub(r'\\d', '', x))\n",
    "data['text'] = data['text'].apply(lambda x: re.sub(r'\\s+', ' ', x))\n",
    "data['text'] = data['text'].apply(lambda x: x.lower())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target                                                                       0\n",
      "id                                                                  1467810369\n",
      "date                                              Mon Apr 06 22:19:45 PDT 2009\n",
      "flag                                                                  NO_QUERY\n",
      "user                                                           _TheSpecialOne_\n",
      "text       awww thats bummer you shoulda got david carr of third day to do it \n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "with pd.option_context('display.max_colwidth', None):\n",
    "    print(data.iloc[0, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/gaeldelescluse/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['text_token'] = 0\n",
    "\n",
    "for i in range(len(data['text'])):\n",
    "    data['text_token'][i] = nltk.word_tokenize(data['text'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(word_list):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_words = []\n",
    "    for word in word_list:\n",
    "        if word not in stop_words:\n",
    "            filtered_words.append(word)\n",
    "    return filtered_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(data['text_token'])):\n",
    "    data['text_token'][i] = remove_stopwords(data['text_token'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target                                                                           0\n",
      "id                                                                      1467810369\n",
      "date                                                  Mon Apr 06 22:19:45 PDT 2009\n",
      "flag                                                                      NO_QUERY\n",
      "user                                                               _TheSpecialOne_\n",
      "text           awww thats bummer you shoulda got david carr of third day to do it \n",
      "text_token            [awww, thats, bummer, shoulda, got, david, carr, third, day]\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "with pd.option_context('display.max_colwidth', None):\n",
    "    print(data.iloc[0, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/gaeldelescluse/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/gaeldelescluse/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/gaeldelescluse/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "nltk.download('punkt')# segmentation phrases\n",
    "nltk.download('averaged_perceptron_tagger') # étiquettes grammaticales\n",
    "nltk.download('wordnet')# synonymes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_words(word_list):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word, get_pos(word)) for word in word_list]\n",
    "    return lemmatized_words\n",
    "\n",
    "def get_pos(word):\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"N\": wordnet.NOUN, \"V\": wordnet.VERB, \"R\": wordnet.ADV, \"J\": wordnet.ADJ}\n",
    "    return tag_dict.get(tag, wordnet.NOUN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(data['text_token'])):\n",
    "    data['text_token'][i] = lemmatize_words(data['text_token'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dataset = '../Data/1.normalized_dataset.csv'\n",
    "data.to_csv(word_dataset, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../Data/1.normalized_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import re\n",
    "\n",
    "array_list = df['text_token'].values\n",
    "data_list = []\n",
    "for item in array_list:\n",
    "    data_list.append(ast.literal_eval(item))\n",
    "\n",
    "df_list = pd.DataFrame({'text_token': data_list})\n",
    "df = df.drop(columns=['text_token'])\n",
    "df['text_token'] = df_list['text_token']\n",
    "df['words'] = df['text_token'].apply(lambda x: ' '.join(x))\n",
    "df = df.drop(columns=['text_token'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target                                                                       0\n",
      "id                                                                  1467810369\n",
      "date                                              Mon Apr 06 22:19:45 PDT 2009\n",
      "flag                                                                  NO_QUERY\n",
      "user                                                           _TheSpecialOne_\n",
      "text       awww thats bummer you shoulda got david carr of third day to do it \n",
      "words                       awww thats bummer shoulda get david carr third day\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "with pd.option_context('display.max_colwidth', None):\n",
    "    print(df.iloc[0, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dataset = '../Data/2.cleaned_dataset.csv'\n",
    "df.to_csv(word_dataset, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../Data/2.cleaned_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>flag</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "      <th>words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2054574443</td>\n",
       "      <td>Sat Jun 06 07:38:48 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>cantyka</td>\n",
       "      <td>saturday night just at home</td>\n",
       "      <td>saturday night home</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>2190983178</td>\n",
       "      <td>Tue Jun 16 04:14:16 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>untitled01</td>\n",
       "      <td>hiccups wont go away squarespace</td>\n",
       "      <td>hiccup wont go away squarespace</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2052335679</td>\n",
       "      <td>Sat Jun 06 00:23:18 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>brittnicarter</td>\n",
       "      <td>tonite was aight ready to go to bed</td>\n",
       "      <td>tonite aight ready go bed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>2178121498</td>\n",
       "      <td>Mon Jun 15 07:13:33 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>NoNo182</td>\n",
       "      <td>busy but good day</td>\n",
       "      <td>busy good day</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2047858869</td>\n",
       "      <td>Fri Jun 05 14:17:18 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>KidLosoLIVE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999995</th>\n",
       "      <td>1</td>\n",
       "      <td>1999040016</td>\n",
       "      <td>Mon Jun 01 19:32:25 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>vinnyvero</td>\n",
       "      <td>its totally worth going unvegan for</td>\n",
       "      <td>totally worth go unvegan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999996</th>\n",
       "      <td>0</td>\n",
       "      <td>2256725354</td>\n",
       "      <td>Sat Jun 20 13:12:37 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>nicroz</td>\n",
       "      <td>forecast rain for the next century bahhh when ...</td>\n",
       "      <td>forecast rain next century bahhh ever go end</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999997</th>\n",
       "      <td>0</td>\n",
       "      <td>2237539564</td>\n",
       "      <td>Fri Jun 19 05:58:17 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Jaden05</td>\n",
       "      <td>ok so like were having some drama with the sho...</td>\n",
       "      <td>ok like drama show guess dont know there gon n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999998</th>\n",
       "      <td>1</td>\n",
       "      <td>2187571504</td>\n",
       "      <td>Mon Jun 15 20:28:47 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>MichelleGrimm</td>\n",
       "      <td>just back from mt olympus wi dells had good da...</td>\n",
       "      <td>back mt olympus wi dell good day kid friend bi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999999</th>\n",
       "      <td>0</td>\n",
       "      <td>2198799953</td>\n",
       "      <td>Tue Jun 16 16:35:19 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>EmDen</td>\n",
       "      <td>now know what the birth of david borrego must...</td>\n",
       "      <td>know birth david borrego must look like never</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000000 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        target          id                          date      flag  \\\n",
       "0            0  2054574443  Sat Jun 06 07:38:48 PDT 2009  NO_QUERY   \n",
       "1            0  2190983178  Tue Jun 16 04:14:16 PDT 2009  NO_QUERY   \n",
       "2            1  2052335679  Sat Jun 06 00:23:18 PDT 2009  NO_QUERY   \n",
       "3            1  2178121498  Mon Jun 15 07:13:33 PDT 2009  NO_QUERY   \n",
       "4            1  2047858869  Fri Jun 05 14:17:18 PDT 2009  NO_QUERY   \n",
       "...        ...         ...                           ...       ...   \n",
       "999995       1  1999040016  Mon Jun 01 19:32:25 PDT 2009  NO_QUERY   \n",
       "999996       0  2256725354  Sat Jun 20 13:12:37 PDT 2009  NO_QUERY   \n",
       "999997       0  2237539564  Fri Jun 19 05:58:17 PDT 2009  NO_QUERY   \n",
       "999998       1  2187571504  Mon Jun 15 20:28:47 PDT 2009  NO_QUERY   \n",
       "999999       0  2198799953  Tue Jun 16 16:35:19 PDT 2009  NO_QUERY   \n",
       "\n",
       "                 user                                               text  \\\n",
       "0             cantyka                       saturday night just at home    \n",
       "1          untitled01                   hiccups wont go away squarespace   \n",
       "2       brittnicarter                tonite was aight ready to go to bed   \n",
       "3             NoNo182                                 busy but good day    \n",
       "4         KidLosoLIVE                                                NaN   \n",
       "...               ...                                                ...   \n",
       "999995      vinnyvero               its totally worth going unvegan for    \n",
       "999996         nicroz  forecast rain for the next century bahhh when ...   \n",
       "999997        Jaden05  ok so like were having some drama with the sho...   \n",
       "999998  MichelleGrimm  just back from mt olympus wi dells had good da...   \n",
       "999999          EmDen   now know what the birth of david borrego must...   \n",
       "\n",
       "                                                    words  \n",
       "0                                     saturday night home  \n",
       "1                         hiccup wont go away squarespace  \n",
       "2                               tonite aight ready go bed  \n",
       "3                                           busy good day  \n",
       "4                                                     NaN  \n",
       "...                                                   ...  \n",
       "999995                           totally worth go unvegan  \n",
       "999996       forecast rain next century bahhh ever go end  \n",
       "999997  ok like drama show guess dont know there gon n...  \n",
       "999998  back mt olympus wi dell good day kid friend bi...  \n",
       "999999      know birth david borrego must look like never  \n",
       "\n",
       "[1000000 rows x 7 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_neg = df[df['target']== 0].sample(500000)\n",
    "df_pos = df[df['target']== 4].sample(500000)\n",
    "df_pos['target'] = 1\n",
    "liste_concat = [df_neg, df_pos]\n",
    "df_sample = pd.concat([df_neg, df_pos], ignore_index=True)\n",
    "df_sample = df_sample.sample(frac=1).reset_index(drop=True)\n",
    "df_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df = '../Data/2.sample_dataset.csv'\n",
    "df_sample.to_csv(sample_df, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Projet7Env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
