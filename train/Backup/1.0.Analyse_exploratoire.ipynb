{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Réalisez une analyse de sentiments grâce au Deep Learning  \n",
    "\n",
    "L'image de marque d'une entreprise est devenue intrinsèquement liée à sa présence et à sa réputation sur les réseaux sociaux. Les plateformes en ligne offrent une tribune sans précédent pour la diffusion d'opinions, qu'elles soient positives ou négatives.  \n",
    "La nécessité pour les entreprises de surveiller activement leur présence numérique et d'anticiper les bad buzz est devenue impérative.  \n",
    "\n",
    "![Air Paradis](https://user.oc-static.com/upload/2023/03/26/16798648691893_Ingenieur-IA-Graphics-P7-01-banner.png) \n",
    "\n",
    "Notre objectif est de fournir à [Air Paradis](https://google.com) un outil puissant et prédictif, capable d'analyser les tendances, de détecter les signaux faibles et de permettre une intervention proactive pour prévenir tout impact négatif sur sa réputation en ligne."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse exploratoire du jeu de données sur l'expression de sentiments de tweets.\n",
    "\n",
    "### Introduction :  \n",
    "\n",
    "\n",
    "L'objectif principal de notre projet est de développer un système intelligent capable d'analyser et de comprendre le sentiment d'un message posté par un utilisateur sur un réseau social, en l'occurence ici [Twitter](https://twitter.com/home?lang=fr). En utilisant des techniques avancées de traitement du langage naturel (NLP), notre solution cherchera à extraire des informations clés, à établir des liens sémantiques et à créer une représentation du sentiment de l'utilisateur."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table des matières\n",
    "\n",
    "\n",
    "#### [1.X](#1.X)\n",
    "#### [2.X](#2.X)\n",
    "#### - [A.X](#A.X)\n",
    "#### - [B.X](#B.X)\n",
    "\n",
    "#### [3.X](#3.X)\n",
    "#### - [A.X](#A.X)\n",
    "#### - [B.X](#B.X)\n",
    "#### - [C.X](#C.X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Présentation des données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import\n",
    "Nous utiliserons une stack de Data Science habituelle : `numpy`, `pandas`, `sklearn`, `matplotlib`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# manipulation des données\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# matplotlib et seaborn pour les représentations graphiques\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "#import missingno as msno\n",
    "\n",
    "# sklearn preprocessing pour le traiter les variables catégorielles\n",
    "#from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Gestion du système de fichiers\n",
    "import os\n",
    "\n",
    "# Suppression des alertes\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les données que nous utiliserons durant notre étude ont le chemin d'accès relatif suivant [\"../Data\"](). Nous pouvons retrouver dans le dossier [./Data]() un fichier csv qui comprend une liste de tweet d'utilisateur. C'est dans ce dossier que nous enregistrerons nos dataframes pour pouvoir les utiliser dans d'autres opérations de notre étude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2.sample_dataset.csv', '2.sample_dataset2.csv', '1.normalized_dataset.csv', 'training1600000.csv', '2.cleaned_dataset.csv']\n"
     ]
    }
   ],
   "source": [
    "print(os.listdir(\"../Data\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous importons le fichier qui nous sera utile pour notre étude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>flag</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599995</th>\n",
       "      <td>4</td>\n",
       "      <td>2193601966</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>AmandaMarie1028</td>\n",
       "      <td>Just woke up. Having no school is the best fee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599996</th>\n",
       "      <td>4</td>\n",
       "      <td>2193601969</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>TheWDBoards</td>\n",
       "      <td>TheWDB.com - Very cool to hear old Walt interv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599997</th>\n",
       "      <td>4</td>\n",
       "      <td>2193601991</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>bpbabe</td>\n",
       "      <td>Are you ready for your MoJo Makeover? Ask me f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599998</th>\n",
       "      <td>4</td>\n",
       "      <td>2193602064</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>tinydiamondz</td>\n",
       "      <td>Happy 38th Birthday to my boo of alll time!!! ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599999</th>\n",
       "      <td>4</td>\n",
       "      <td>2193602129</td>\n",
       "      <td>Tue Jun 16 08:40:50 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>RyanTrevMorris</td>\n",
       "      <td>happy #charitytuesday @theNSPCC @SparksCharity...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1600000 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         target          id                          date      flag  \\\n",
       "0             0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY   \n",
       "1             0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   \n",
       "2             0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY   \n",
       "3             0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "4             0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "...         ...         ...                           ...       ...   \n",
       "1599995       4  2193601966  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
       "1599996       4  2193601969  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
       "1599997       4  2193601991  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
       "1599998       4  2193602064  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
       "1599999       4  2193602129  Tue Jun 16 08:40:50 PDT 2009  NO_QUERY   \n",
       "\n",
       "                    user                                               text  \n",
       "0        _TheSpecialOne_  @switchfoot http://twitpic.com/2y1zl - Awww, t...  \n",
       "1          scotthamilton  is upset that he can't update his Facebook by ...  \n",
       "2               mattycus  @Kenichan I dived many times for the ball. Man...  \n",
       "3                ElleCTF    my whole body feels itchy and like its on fire   \n",
       "4                 Karoli  @nationwideclass no, it's not behaving at all....  \n",
       "...                  ...                                                ...  \n",
       "1599995  AmandaMarie1028  Just woke up. Having no school is the best fee...  \n",
       "1599996      TheWDBoards  TheWDB.com - Very cool to hear old Walt interv...  \n",
       "1599997           bpbabe  Are you ready for your MoJo Makeover? Ask me f...  \n",
       "1599998     tinydiamondz  Happy 38th Birthday to my boo of alll time!!! ...  \n",
       "1599999   RyanTrevMorris  happy #charitytuesday @theNSPCC @SparksCharity...  \n",
       "\n",
       "[1600000 rows x 6 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('../Data/training1600000.csv', encoding='latin-1', header=None)\n",
    "data = data.rename(columns={data.columns[0]: 'target'})\n",
    "data = data.rename(columns={data.columns[1]: 'id'})\n",
    "data = data.rename(columns={data.columns[2]: 'date'})\n",
    "data = data.rename(columns={data.columns[3]: 'flag'})\n",
    "data = data.rename(columns={data.columns[4]: 'user'})\n",
    "data = data.rename(columns={data.columns[5]: 'text'})\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le fichier contient 1.600.000 tweets d'utilisateurs extrait en utilisant l'API de Twitter. Les tweets ont été annoté (0 = negatif, 4 = positif) et peuvent être utilisé pour détecter le sentiment d'un tweet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le DataFrame est caractérisé par 6 variables :  \n",
    "\n",
    "1. target: le sentiment du tweet (0 = negatif, 4 = positif)\n",
    "2. id: identifiant du tweet ( 2087)\n",
    "3. date: date du tweet (Sat May 16 23:58:44 UTC 2009)\n",
    "4. flag: La requête (LyX). Si il n'y a pas de requête alors la valeur sera NO_QUERY\n",
    "5. user: l'utilisateur qui a tweeté (robotickilldozr)\n",
    "6. text: le texte du tweet (avecLyX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target                                                                                                                      0\n",
      "id                                                                                                                 1467810369\n",
      "date                                                                                             Mon Apr 06 22:19:45 PDT 2009\n",
      "flag                                                                                                                 NO_QUERY\n",
      "user                                                                                                          _TheSpecialOne_\n",
      "text      @switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "with pd.option_context('display.max_colwidth', None):\n",
    "    print(data.iloc[0, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "data['text'] = data['text'].apply(lambda x: re.sub(r'\\S*@\\S*\\s?', '', x))\n",
    "data['text'] = data['text'].apply(lambda x: re.sub(r'http\\S+', '', x))\n",
    "data['text'] = data['text'].apply(lambda x: re.sub(r'[^\\w\\s]', '', x))\n",
    "data['text'] = data['text'].apply(lambda x: re.sub(r'\\b\\w\\b', '', x))\n",
    "data['text'] = data['text'].apply(lambda x: re.sub(r'\\d', '', x))\n",
    "data['text'] = data['text'].apply(lambda x: re.sub(r'\\s+', ' ', x))\n",
    "data['text'] = data['text'].apply(lambda x: x.lower())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target                                                                       0\n",
      "id                                                                  1467810369\n",
      "date                                              Mon Apr 06 22:19:45 PDT 2009\n",
      "flag                                                                  NO_QUERY\n",
      "user                                                           _TheSpecialOne_\n",
      "text       awww thats bummer you shoulda got david carr of third day to do it \n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "with pd.option_context('display.max_colwidth', None):\n",
    "    print(data.iloc[0, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/gaeldelescluse/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['text_token'] = 0\n",
    "\n",
    "for i in range(len(data['text'])):\n",
    "    data['text_token'][i] = nltk.word_tokenize(data['text'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(word_list):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_words = []\n",
    "    for word in word_list:\n",
    "        if word not in stop_words:\n",
    "            filtered_words.append(word)\n",
    "    return filtered_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(data['text_token'])):\n",
    "    data['text_token'][i] = remove_stopwords(data['text_token'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target                                                                           0\n",
      "id                                                                      1467810369\n",
      "date                                                  Mon Apr 06 22:19:45 PDT 2009\n",
      "flag                                                                      NO_QUERY\n",
      "user                                                               _TheSpecialOne_\n",
      "text           awww thats bummer you shoulda got david carr of third day to do it \n",
      "text_token            [awww, thats, bummer, shoulda, got, david, carr, third, day]\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "with pd.option_context('display.max_colwidth', None):\n",
    "    print(data.iloc[0, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/gaeldelescluse/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/gaeldelescluse/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/gaeldelescluse/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "nltk.download('punkt')# segmentation phrases\n",
    "nltk.download('averaged_perceptron_tagger') # étiquettes grammaticales\n",
    "nltk.download('wordnet')# synonymes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_words(word_list):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word, get_pos(word)) for word in word_list]\n",
    "    return lemmatized_words\n",
    "\n",
    "def get_pos(word):\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"N\": wordnet.NOUN, \"V\": wordnet.VERB, \"R\": wordnet.ADV, \"J\": wordnet.ADJ}\n",
    "    return tag_dict.get(tag, wordnet.NOUN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(data['text_token'])):\n",
    "    data['text_token'][i] = lemmatize_words(data['text_token'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dataset = '../Data/1.normalized_dataset.csv'\n",
    "data.to_csv(word_dataset, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../Data/1.normalized_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import re\n",
    "\n",
    "array_list = df['text_token'].values\n",
    "data_list = []\n",
    "for item in array_list:\n",
    "    data_list.append(ast.literal_eval(item))\n",
    "\n",
    "df_list = pd.DataFrame({'text_token': data_list})\n",
    "df = df.drop(columns=['text_token'])\n",
    "df['text_token'] = df_list['text_token']\n",
    "df['words'] = df['text_token'].apply(lambda x: ' '.join(x))\n",
    "df = df.drop(columns=['text_token'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target                                                                       0\n",
      "id                                                                  1467810369\n",
      "date                                              Mon Apr 06 22:19:45 PDT 2009\n",
      "flag                                                                  NO_QUERY\n",
      "user                                                           _TheSpecialOne_\n",
      "text       awww thats bummer you shoulda got david carr of third day to do it \n",
      "words                       awww thats bummer shoulda get david carr third day\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "with pd.option_context('display.max_colwidth', None):\n",
    "    print(df.iloc[0, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dataset = '../Data/2.cleaned_dataset.csv'\n",
    "df.to_csv(word_dataset, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Projet7Env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
