{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-25 11:13:53.406347: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from gensim.models import Word2Vec\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>flag</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "      <th>words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2054665466</td>\n",
       "      <td>Sat Jun 06 07:50:24 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>boring_alice</td>\n",
       "      <td>ive had awesome day but the sun is missing wan...</td>\n",
       "      <td>ive awesome day sun miss want well weather</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1823190427</td>\n",
       "      <td>Sat May 16 21:20:00 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>yassychan</td>\n",
       "      <td>you will do great saw kevin teaching you</td>\n",
       "      <td>great saw kevin teach</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1826975071</td>\n",
       "      <td>Sun May 17 09:43:20 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>PRNCSmuriel3</td>\n",
       "      <td>its cold in md too</td>\n",
       "      <td>cold md</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>2202406793</td>\n",
       "      <td>Tue Jun 16 21:44:37 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mariazimmerman</td>\n",
       "      <td>does anyone know the girl that died of swine f...</td>\n",
       "      <td>anyone know girl die swine flu maybe go ucsd f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>2242106714</td>\n",
       "      <td>Fri Jun 19 11:46:38 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>katrinachelsea</td>\n",
       "      <td>watching amelie and wishing was french</td>\n",
       "      <td>watch amelie wish french</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999995</th>\n",
       "      <td>1</td>\n",
       "      <td>1679383266</td>\n",
       "      <td>Sat May 02 09:13:22 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Shawna1976</td>\n",
       "      <td>goodmorning jordan needs to talk to you</td>\n",
       "      <td>goodmorning jordan need talk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999996</th>\n",
       "      <td>1</td>\n",
       "      <td>2053651702</td>\n",
       "      <td>Sat Jun 06 05:16:26 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>harlequinxgirl</td>\n",
       "      <td>good mprninh to you too</td>\n",
       "      <td>good mprninh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999997</th>\n",
       "      <td>0</td>\n",
       "      <td>1563449159</td>\n",
       "      <td>Sun Apr 19 22:52:58 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Cynthi_ocho</td>\n",
       "      <td>cant believe spring break is coming to an end</td>\n",
       "      <td>cant believe spring break come end</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999998</th>\n",
       "      <td>0</td>\n",
       "      <td>1751244042</td>\n",
       "      <td>Sat May 09 19:00:02 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Ericanderson09</td>\n",
       "      <td>everyone have texted in the last hour complete...</td>\n",
       "      <td>everyone texted last hour completely ignore im...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999999</th>\n",
       "      <td>0</td>\n",
       "      <td>2296388705</td>\n",
       "      <td>Tue Jun 23 09:07:21 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ErinLeBron</td>\n",
       "      <td>awwww maybe he never had any to teach him the ...</td>\n",
       "      <td>awwww maybe never teach proper dresscodes gd look</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>995491 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        target          id                          date      flag  \\\n",
       "0            0  2054665466  Sat Jun 06 07:50:24 PDT 2009  NO_QUERY   \n",
       "1            1  1823190427  Sat May 16 21:20:00 PDT 2009  NO_QUERY   \n",
       "2            0  1826975071  Sun May 17 09:43:20 PDT 2009  NO_QUERY   \n",
       "3            0  2202406793  Tue Jun 16 21:44:37 PDT 2009  NO_QUERY   \n",
       "4            0  2242106714  Fri Jun 19 11:46:38 PDT 2009  NO_QUERY   \n",
       "...        ...         ...                           ...       ...   \n",
       "999995       1  1679383266  Sat May 02 09:13:22 PDT 2009  NO_QUERY   \n",
       "999996       1  2053651702  Sat Jun 06 05:16:26 PDT 2009  NO_QUERY   \n",
       "999997       0  1563449159  Sun Apr 19 22:52:58 PDT 2009  NO_QUERY   \n",
       "999998       0  1751244042  Sat May 09 19:00:02 PDT 2009  NO_QUERY   \n",
       "999999       0  2296388705  Tue Jun 23 09:07:21 PDT 2009  NO_QUERY   \n",
       "\n",
       "                  user                                               text  \\\n",
       "0         boring_alice  ive had awesome day but the sun is missing wan...   \n",
       "1            yassychan          you will do great saw kevin teaching you    \n",
       "2         PRNCSmuriel3                                its cold in md too    \n",
       "3       mariazimmerman  does anyone know the girl that died of swine f...   \n",
       "4       katrinachelsea            watching amelie and wishing was french    \n",
       "...                ...                                                ...   \n",
       "999995      Shawna1976           goodmorning jordan needs to talk to you    \n",
       "999996  harlequinxgirl                           good mprninh to you too    \n",
       "999997     Cynthi_ocho     cant believe spring break is coming to an end    \n",
       "999998  Ericanderson09  everyone have texted in the last hour complete...   \n",
       "999999      ErinLeBron  awwww maybe he never had any to teach him the ...   \n",
       "\n",
       "                                                    words  \n",
       "0              ive awesome day sun miss want well weather  \n",
       "1                                   great saw kevin teach  \n",
       "2                                                 cold md  \n",
       "3       anyone know girl die swine flu maybe go ucsd f...  \n",
       "4                                watch amelie wish french  \n",
       "...                                                   ...  \n",
       "999995                       goodmorning jordan need talk  \n",
       "999996                                       good mprninh  \n",
       "999997                 cant believe spring break come end  \n",
       "999998  everyone texted last hour completely ignore im...  \n",
       "999999  awwww maybe never teach proper dresscodes gd look  \n",
       "\n",
       "[995491 rows x 7 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../Data/2.sample_dataset.csv\")\n",
    "df = df.dropna(subset=['words'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(796392,)\n",
      "(796392,)\n",
      "(199099,)\n",
      "(199099,)\n"
     ]
    }
   ],
   "source": [
    "X = df['words']\n",
    "y = df['target']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from gensim.models import Word2Vec\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense\n",
    "import numpy as np\n",
    "\n",
    "# Charger votre DataFrame\n",
    "# df = ...\n",
    "\n",
    "# Divisez le DataFrame en ensembles d'entraînement et de test\n",
    "train_data, test_data = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Tokenisation des textes avec des mots\n",
    "tokenized_texts = [text.split() for text in train_data['words']]\n",
    "\n",
    "# Entraînement du modèle Word2Vec\n",
    "embedding_dim = 100\n",
    "word2vec_model = Word2Vec(sentences=tokenized_texts, vector_size=embedding_dim, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Tokenisation des textes\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(train_data['words'])\n",
    "\n",
    "# Convertir les textes en séquences d'entiers\n",
    "train_sequences = tokenizer.texts_to_sequences(train_data['words'])\n",
    "test_sequences = tokenizer.texts_to_sequences(test_data['words'])\n",
    "\n",
    "# Rembourrage des séquences pour avoir des tailles égales\n",
    "max_length = max(max(len(seq) for seq in train_sequences), max(len(seq) for seq in test_sequences))\n",
    "train_data_pad = pad_sequences(train_sequences, maxlen=max_length, padding='post')\n",
    "test_data_pad = pad_sequences(test_sequences, maxlen=max_length, padding='post')\n",
    "\n",
    "# Construction de la matrice d'embedding à partir du modèle Word2Vec\n",
    "embedding_matrix = []\n",
    "for word, index in tokenizer.word_index.items():\n",
    "    if word in word2vec_model.wv:\n",
    "        embedding_matrix.append(word2vec_model.wv[word])\n",
    "    else:\n",
    "        embedding_matrix.append([0] * embedding_dim)\n",
    "\n",
    "embedding_matrix = np.array(embedding_matrix)\n",
    "\n",
    "# Création du modèle CNN avec la matrice d'embedding pré-entraînée\n",
    "model_word2vec = Sequential()\n",
    "model_word2vec.add(Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=embedding_dim, input_length=max_length, weights=[embedding_matrix], trainable=False))\n",
    "model_word2vec.add(Conv1D(128, 5, activation='relu'))\n",
    "model_word2vec.add(GlobalMaxPooling1D())\n",
    "model_word2vec.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compiler le modèle\n",
    "model_word2vec.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Définir les labels d'entraînement et de test\n",
    "train_labels = train_data['sentiment'].values\n",
    "test_labels = test_data['sentiment'].values\n",
    "\n",
    "# Entraîner le modèle\n",
    "model_word2vec.fit(train_data_pad, train_labels, epochs=5, validation_data=(test_data_pad, test_labels))\n",
    "\n",
    "# Évaluation sur l'ensemble de test\n",
    "loss, accuracy = model_word2vec.evaluate(test_data_pad, test_labels)\n",
    "print(f\"Test Accuracy: {accuracy*100:.2f}%\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Projet7Env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
